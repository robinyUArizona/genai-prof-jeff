{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_07_2_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 7: LangChain: Agents**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 7 Material\n",
    "\n",
    "* Part 7.1: Introduction to LangChain Agents [[Video]](https://www.youtube.com/watch?v=J5Vr___lSSs) [[Notebook]](t81_559_class_07_1_agents.ipynb)\n",
    "* **Part 7.2: Understanding LangChain Agent Tools** [[Video]](https://www.youtube.com/watch?v=qMquBmteYw4) [[Notebook]](t81_559_class_07_2_tools.ipynb)\n",
    "* Part 7.3: LangChain Retrival and Search Tools [[Video]](https://www.youtube.com/watch?v=NB5qGPLoBBE) [[Notebook]](t81_559_class_07_3_search_tools.ipynb)\n",
    "* Part 7.4: Constructing LangChain Agents [[Video]](https://www.youtube.com/watch?v=OJe5oHvrdHk) [[Notebook]](t81_559_class_07_4_more_agent.ipynb)\n",
    "* Part 7.5: Custom Agents [[Video]](https://www.youtube.com/watch?v=IsJemVYSEdc) [[Notebook]](t81_559_class_07_5_custom_agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "49979b55-7f83-4c13-ebfc-fc957c569c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai langchain_experimental duckduckgo-search langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 7.2: LangChain Agent Tools\n",
    "\n",
    "\n",
    "LangChain agents are versatile entities designed to perform specific tasks autonomously. Central to their functionality are [tools](https://python.langchain.com/v0.1/docs/modules/tools/), which are specialized components that agents can utilize to accomplish their objectives. These tools can range from data retrieval and processing utilities to interactive interfaces for user engagement. By leveraging these tools, LangChain agents can efficiently execute complex workflows, automate routine tasks, and provide intelligent solutions tailored to user needs. Whether it's querying databases, parsing documents, or interacting with APIs, the strategic use of tools enables LangChain agents to enhance productivity and deliver precise outcomes.\n",
    "\n",
    "\n",
    "Large Language Models (LLMs) inherently lack access to real-time information such as the current date and time, stock market data, and breaking news. This limitation stems from their design, which relies on pre-existing datasets that do not include ongoing updates. Additionally, LLMs are not equipped to perform mathematical calculations directly, which can restrict their utility in scenarios requiring precise numerical operations. To overcome these constraints, LLMs can employ specialized tools. For instance, search engine tools enable LLMs to retrieve the latest information from the web, ensuring up-to-date responses. Similarly, tools designed for mathematical computation can assist LLMs in accurately processing and solving mathematical problems, thereby enhancing their overall capability and accuracy.\n",
    "\n",
    "* [Available Langchain Tools](https://python.langchain.com/v0.2/docs/integrations/tools/)\n",
    "* [Available Langchain Toolkits](https://python.langchain.com/v0.2/docs/integrations/toolkits/)\n",
    "\n",
    "## Tools for Math\n",
    "\n",
    "\n",
    "We will begin our exploration of LangChain tools by using a tool specifically designed for math. Large Language Models (LLMs), like ChatGPT, are generally poor at performing mathematical calculations without the assistance of specialized tools. This is because LLMs are trained primarily on textual data and lack the precision required for accurate arithmetic operations. Consequently, they often produce inaccurate results when asked to perform math independently. To see how a LLM actually performs mathematics, I asked ChatGPT and it gave a decent high-level summary.\n",
    "\n",
    "> Large Language Models (LLMs), like me, do not inherently perform arithmetic calculations the same way a calculator or dedicated algorithm would. Instead, we generate responses based on patterns in the data we've been trained on. Here's a simplified explanation of how we handle such tasks:\n",
    ">\n",
    "> Pattern Recognition: During training, LLMs are exposed to vast amounts of text data, which includes examples of arithmetic and mathematical reasoning. We learn patterns and structures in these examples, enabling us to approximate calculations.\n",
    ">\n",
    "> Token Prediction: When asked to perform a calculation, an LLM doesn't actually \"calculate\" in the traditional sense. Instead, it predicts the most likely sequence of tokens (numbers, in this case) that should follow based on the input. This prediction is influenced by the training data but does not involve real arithmetic operations.\n",
    ">\n",
    "> Approximation and Heuristics: For smaller or simpler calculations, the model might generate the correct answer because it has seen enough examples during training. For larger or more complex calculations, the model might generate an approximate answer or even make a guess based on learned patterns.\n",
    ">\n",
    "> For example, if you ask an LLM to multiply 872947493 by 7492374932, it will try to generate a plausible sequence of digits based on what it has seen in the training data, but this sequence is unlikely to be correct without an actual computational algorithm.\n",
    ">\n",
    ">Hereâ€™s a brief comparison of how a traditional method (e.g., a calculator or algorithm) and an LLM approach such a problem:\n",
    ">\n",
    "> * Traditional Method: Uses precise algorithms to perform each step of the multiplication (e.g., long multiplication or fast algorithms like the Karatsuba algorithm).\n",
    "> * LLM Method: Predicts the next sequence of digits based on patterns and probabilities from the training data.\n",
    "So, while an LLM might \"attempt\" to give an answer, it lacks the precision and algorithmic foundation to guarantee accuracy for complex arithmetic without dedicated computational tools.\n",
    "\n",
    "To see this in action, let's ask an LLM to perform a mathematical operation. We will choose numbers that were unlikely in the LLM's training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5x_qovp1E13",
    "outputId": "064aabbf-fd90-495d-db7d-3e984214bb2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='8273 times 1821 equals 15,086,213.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 17, 'total_tokens': 31}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None} id='run-3a675778-1103-4384-8350-8082c63cca8e-0' usage_metadata={'input_tokens': 17, 'output_tokens': 14, 'total_tokens': 31}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.2,\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "print(llm.invoke(\"What is 8273 times 1821?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4ilDY5oB4dT"
   },
   "source": [
    "The resulting number appears reasonable, but that's the point. LLMs are trained to produce believable results, not necessarily correct ones. To verify the LLM, we'll have Python perform this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-4t2gN51jom",
    "outputId": "69b84bf5-a6db-4444-c8ab-4ba4811e15b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15065133\n"
     ]
    }
   ],
   "source": [
    "print(8273 * 1821)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXJis_ZV1qpl",
    "outputId": "4a836215-91d7-45ba-c066-b59aaabe21b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9700\n"
     ]
    }
   ],
   "source": [
    "print( abs(15065133 - 15055433 ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t2lZcP1CVT9"
   },
   "source": [
    "We can see that the LLM was several thousand off. Unfortunately, LangChain does not include a calculator tool, at least as of summer 2024. We will look at two approaches. First, we will use the LangChain-provided PythonREPL. The following code shows how to use PythonREPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "u3O0SkxSwPFk",
    "outputId": "1c4f8341-cd07-446f-d5d9-2d352177138f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "python_repl.run(\"print(1+1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MO5-3uBFvqO"
   },
   "source": [
    "It begins by importing necessary modules and classes from the LangChain, LangChain OpenAI, and LangChain Community libraries.\n",
    "A tool for executing Python commands (repl_tool) is created using the Tool class, with a description indicating its purpose as a Python shell that requires valid Python commands. The function python_repl.run is assigned to execute the commands.\n",
    "\n",
    "The prompt for the agent is pulled from a hub, specified by the identifier \"hwchase17/openai-functions-agent\".\n",
    "\n",
    "An agent is then created using the create_tool_calling_agent function, which takes the language model (llm), the tools (here, just repl_tool), and the prompt. This agent is wrapped in an AgentExecutor with the verbose mode enabled to provide detailed logs of the execution process.\n",
    "Finally, the agent is invoked with an input command to calculate the product of 8273 and 1821."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5UI3eL30iEw",
    "outputId": "2b7d4f21-3932-4f57-956d-ba4e94e4ed68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:312: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:5515: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl` with `{'input': '8273 * 1821'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m15065133\u001b[0m\u001b[32;1m\u001b[1;3mThe result of \\( 8273 \\times 1821 \\) is \\( 15,065,133 \\).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is 8273 * 1821?', 'output': 'The result of \\\\( 8273 \\\\times 1821 \\\\) is \\\\( 15,065,133 \\\\).'}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools import StructuredTool\n",
    "from pydantic import BaseModel\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    temperature=0.2,\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Define a Pydantic schema for the input arguments\n",
    "class PythonReplInput(BaseModel):\n",
    "    input: str\n",
    "\n",
    "# Define the tool using StructuredTool with an args_schema\n",
    "def run_python_code(input: str, **kwargs):\n",
    "    try:\n",
    "        # Safely evaluate the input string as a Python expression\n",
    "        result = eval(input)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "repl_tool = StructuredTool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=run_python_code,  # Use the custom Python REPL function\n",
    "    args_schema=PythonReplInput  # Define the expected input schema\n",
    ")\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "tools = [repl_tool]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Pass a string Python command as input\n",
    "result = agent_executor.invoke({\"input\": \"What is 8273 * 1821?\"})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iL2vLrGYDL"
   },
   "source": [
    "## Create a Cusom Math Tool\n",
    "\n",
    "The Python REPL tool we just used can execute any Python command. Therefore, it can be a security concern if we only wish to perform math calculations. In this section, we will see how to create a custom tool that can only perform basic math calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0Fux9SZGfu0",
    "outputId": "69e761a2-3fc8-4917-b8df-c3046a9e3f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-34b3ab86af62>:38: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  result = chain(inputs)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "import numpy as np\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "class SafeCalculator:\n",
    "    def calculate(self, expression):\n",
    "        try:\n",
    "            # Evaluate the mathematical expression using NumPy\n",
    "            result = eval(expression, {\"__builtins__\": None}, {\"np\": np})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "\n",
    "class CalculatorChain(Chain):\n",
    "    calculator: SafeCalculator\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        expression = inputs[\"expression\"]\n",
    "        result = self.calculator.calculate(expression)\n",
    "        return {\"result\": result}\n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return [\"expression\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self):\n",
    "        return [\"result\"]\n",
    "\n",
    "# Initialize the safe calculator tool\n",
    "safe_calculator = SafeCalculator()\n",
    "\n",
    "# Example usage\n",
    "chain = CalculatorChain(calculator=safe_calculator)\n",
    "expression = \"3 * (2 + 5) / 7\"\n",
    "inputs = {\"expression\": expression}\n",
    "result = chain(inputs)\n",
    "print(f\"Result: {result['result']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VN_jww59Ick9",
    "outputId": "f1a6aae6-5e50-4cdd-a22e-c61231f43484"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:312: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `safe_calc` with `{'input': '8273 * 1821'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m15065133\u001b[0m\u001b[32;1m\u001b[1;3mThe result of \\( 8273 \\times 1821 \\) is \\( 15,065,133 \\).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '8273 * 1821', 'output': 'The result of \\\\( 8273 \\\\times 1821 \\\\) is \\\\( 15,065,133 \\\\).'}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools import StructuredTool\n",
    "from pydantic import BaseModel\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    temperature=0.2,\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Define a Pydantic schema for the input arguments\n",
    "class MathExpressionInput(BaseModel):\n",
    "    input: str\n",
    "\n",
    "# Define a safe calculator function\n",
    "def safe_calculator(input: str, **kwargs):\n",
    "    try:\n",
    "        # Safely evaluate the input math expression\n",
    "        result = eval(input)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Define the tool using StructuredTool with args_schema\n",
    "safe_math_tool = StructuredTool(\n",
    "    name=\"safe_calc\",\n",
    "    description=\"A math calculator used to evaluate mathematical expressions. Input should be a valid math expression, similar to Python.\",\n",
    "    func=safe_calculator,  # Use the safe calculator function\n",
    "    args_schema=MathExpressionInput  # Define the expected input schema\n",
    ")\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "tools = [safe_math_tool]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Pass a mathematical expression as input\n",
    "result = agent_executor.invoke({\"input\": \"8273 * 1821\"})\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
