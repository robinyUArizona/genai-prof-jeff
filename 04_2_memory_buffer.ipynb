{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_04_2_memory_buffer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 4: LangChain: Chat and Memory**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 4 Material\n",
    "\n",
    "* Part 4.1: LangChain Conversations [[Video]](https://www.youtube.com/watch?v=Effbhxq07Ag) [[Notebook]](t81_559_class_04_1_langchain_chat.ipynb)\n",
    "* **Part 4.2: Conversation Buffer Window Memory** [[Video]](https://www.youtube.com/watch?v=14RgiFVGfAA) [[Notebook]](t81_559_class_04_2_memory_buffer.ipynb)\n",
    "* Part 4.3: Conversation Token Buffer Memory [[Video]](https://www.youtube.com/watch?v=QTe5g2c3bSM) [[Notebook]](t81_559_class_04_3_memory_token.ipynb)\n",
    "* Part 4.4: Conversation Summary Memory [[Video]](https://www.youtube.com/watch?v=asZQ8Ktqmt8) [[Notebook]](t81_559_class_04_4_memory_summary.ipynb)\n",
    "* Part 4.5: Persisting Langchain Memory [[Video]](https://www.youtube.com/watch?v=sjCyqqOQcPA) [[Notebook]](t81_559_class_04_5_memory_persist.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "ad8b02ba-a203-41c3-a27a-1db0cf2e4ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 4.2: Conversation Buffer Window Memory\n",
    "\n",
    "We previously saw that we could build up an LLM chat client by building an ever-increasing script of what the human and AI said in the conversation. We constantly add the human response and wait to see what the AI will respond to next. This cycle continues as long as the chat.\n",
    "\n",
    "This ever-increasing chat memory is a typical pattern for LLMs, and as a result, LangChain has several predefined Python objects that allow you to implement this sort of memory-based chatbot.\n",
    "\n",
    "## Creating a Chat Conversation\n",
    "\n",
    "ConversationChain in LangChain is a framework designed to facilitate the development and management of conversational AI systems. It primarily orchestrates the interaction between different components, such as the dialogue management system, language models, and various types of memory that retain information from the conversation to improve response relevance and coherence.\n",
    "\n",
    "In this context, memory types play a crucial role. They help the system remember and utilize past interactions to maintain context and enhance the continuity of the conversation. This system frees us from tracking the conversation as performed manually in the pervious section.\n",
    "\n",
    "We first examine ConversationBufferWindowMemory is a dynamic memory model designed to manage the flow of a dialogue by retaining a record of the last K interactions within a conversation. This method ensures that the memory buffer maintains a manageable size, avoiding overflow and performance issues that can arise with an excessively large interaction history. By focusing on the most recent interactions, this memory type effectively creates a \"sliding window\" that continuously updates as new exchanges occur, allowing for contextually relevant responses while efficiently managing system resources. This approach is particularly beneficial in applications where maintaining an immediate and context-aware dialogue is crucial, such as in customer service bots or interactive learning tools.\n",
    "\n",
    "The provided code snippet demonstrates how to set up and use a conversational AI system using the LangChain framework, specifically focusing on maintaining a concise memory of recent interactions. The ConversationChain class orchestrates the conversation flow, integrating various components such as language model, memory, and prompt formatting.\n",
    "\n",
    "Firstly, several modules are imported, including ConversationChain for managing the conversation, ConversationBufferWindowMemory for handling the memory of recent interactions, and ChatOpenAI to interface with OpenAI's language models. The PromptTemplate is used to define the structure of the prompts sent to the language model.\n",
    "\n",
    "In the setup, MODEL specifies the particular version of the language model (gpt-4o-mini), and TEMPLATE outlines the format of the conversation, integrating previous dialogue history and the current user input into the prompt. This structured prompt is then used to create a PromptTemplate object.\n",
    "\n",
    "The begin_conversation function initializes the language model and sets up the memory to store the last five interactions (k=5). This sliding window of memory helps keep the conversation relevant and efficient by focusing on the most recent exchanges. The ConversationChain object is created with the prompt template, the language model, and the memory buffer, and it's ready to process conversation inputs.\n",
    "\n",
    "The converse function takes a conversation object and a user prompt to produce responses. It uses the conversation object's invoke method, which applies the prompt template, consults the memory, and generates a response based on the current and recent dialogue context.\n",
    "\n",
    "Overall, this setup is optimized for creating a responsive and context-aware conversational AI that doesn't overload with too much past information, maintaining relevancy and efficiency in ongoing interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import PromptTemplate\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "TEMPLATE = \"\"\"You are a helpful assistant. Format answers with markdown.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)\n",
    "\n",
    "def begin_conversation():\n",
    "    # Initialize the OpenAI LLM with your API key\n",
    "    llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.0,\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "    # Initialize memory and conversation\n",
    "    memory = ConversationBufferWindowMemory(k=5)\n",
    "    conversation = ConversationChain(\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return conversation\n",
    "\n",
    "def converse(conversation, prompt):\n",
    "    output = conversation.invoke(prompt)\n",
    "    return output['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClPhLkGldhPt"
   },
   "source": [
    "We can now carry on a simple conversation with the LLM, using LangChain to track the conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "ydaqwgRiH4D6",
    "outputId": "b9c6d85c-918f-48a8-bf61-dd49837d0337"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yrobi\\AppData\\Local\\Temp\\ipykernel_9816\\3827138014.py:26: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I don't know your name. How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello, Jeff! How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff! How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "# Initialize the OpenAI LLM with your API key\n",
    "llm = ChatOpenAI(\n",
    "  model=MODEL,\n",
    "  temperature= 0.3,\n",
    "  n= 1)\n",
    "\n",
    "conversation = begin_conversation()\n",
    "output = converse(conversation, \"Hello, what is my name?\")\n",
    "display_markdown(output,raw=True)\n",
    "output = converse(conversation, \"Oh sorry, my name is Jeff.\")\n",
    "display_markdown(output,raw=True)\n",
    "output = converse(conversation, \"What is my name?\")\n",
    "display_markdown(output,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulYUrfVcnlgF"
   },
   "source": [
    "## Conversing with the LLM in Markdown\n",
    "\n",
    "Just as before, we can request that the LLM output be in mardown. This allows code and tables to be represented clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MHL6Ik8IM2PA"
   },
   "outputs": [],
   "source": [
    "def chat(conversation, prompt):\n",
    "  print(f\"Human: {prompt}\")\n",
    "  output = converse(conversation, prompt)\n",
    "  display_markdown(output,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvzsYl3Qplac"
   },
   "source": [
    "The provided code sequence demonstrates a conversation between a human user and a Large Language Model (LLM), making use of the chat function to interactively manage the conversation and display responses in Markdown format. This approach allows for a dynamic and contextually aware chat, while also enhancing the visual and structural presentation of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "PtLDak7TM_FU",
    "outputId": "74b1a787-f0bd-4add-c481-6bfe5603cb96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm sorry, but I don't know your name. How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Jeff! How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff! How can I assist you further?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Give me a table of the 5 most populus cities with population and country.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here’s a table of the 5 most populous cities along with their population and country:\n",
       "\n",
       "| City            | Population (approx.) | Country         |\n",
       "|------------------|---------------------|------------------|\n",
       "| Tokyo            | 37.4 million        | Japan            |\n",
       "| Delhi            | 31.0 million        | India            |\n",
       "| Shanghai         | 27.0 million        | China            |\n",
       "| São Paulo        | 22.0 million        | Brazil           |\n",
       "| Mexico City      | 21.7 million        | Mexico           |\n",
       "\n",
       "If you need more information or assistance, feel free to ask!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = begin_conversation()\n",
    "chat(conversation, \"What is my name?\")\n",
    "chat(conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(conversation, \"What is my name?\")\n",
    "chat(conversation, \"Give me a table of the 5 most populus cities with population and country.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeY-9YSOno_t"
   },
   "source": [
    "## Constraining the Conversation with a System Prompt\n",
    "\n",
    "You can use the system prompt to constrain the conversation to a specific topic. Here, we provide a simple agent that will only discuss life insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "NyGpLJmWjGNq",
    "outputId": "9d2b3222-0ca4-434f-ab78-56f928bea144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm here to help you with questions about life insurance. How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm glad to meet you, Jeff! How can I assist you with your life insurance questions today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm here to help you with questions about life insurance, Jeff. How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is your favorite programming language?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm here to help you with questions about life insurance. How can I assist you today, Jeff?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is the difference between a term and whole life policy?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Difference Between Term and Whole Life Insurance\n",
       "\n",
       "**Term Life Insurance:**\n",
       "- **Duration:** Provides coverage for a specific period (e.g., 10, 20, or 30 years).\n",
       "- **Premiums:** Generally lower premiums compared to whole life.\n",
       "- **Payout:** Pays a death benefit only if the insured passes away during the term.\n",
       "- **Cash Value:** Does not accumulate cash value.\n",
       "\n",
       "**Whole Life Insurance:**\n",
       "- **Duration:** Provides coverage for the entire lifetime of the insured, as long as premiums are paid.\n",
       "- **Premiums:** Higher premiums than term life, but they remain level throughout the life of the policy.\n",
       "- **Payout:** Guarantees a death benefit regardless of when the insured passes away.\n",
       "- **Cash Value:** Accumulates cash value over time, which can be borrowed against or withdrawn.\n",
       "\n",
       "If you have more questions about life insurance, feel free to ask, Jeff!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import PromptTemplate\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "\n",
    "def begin_conversation_insurance():\n",
    "    TEMPLATE = \"\"\"You are a helpful agent to answer questions about life insurance. Do not talk\n",
    "    about anything else with users. . Format answers with markdown.\n",
    "\n",
    "    Current conversation:\n",
    "    {history}\n",
    "    Human: {input}\n",
    "    AI:\"\"\"\n",
    "    PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)\n",
    "\n",
    "    # Initialize the OpenAI LLM with your API key\n",
    "    llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.0,\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "    # Initialize memory and conversation\n",
    "    memory = ConversationBufferWindowMemory(k=5)\n",
    "    conversation = ConversationChain(\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return conversation\n",
    "\n",
    "conversation = begin_conversation_insurance()\n",
    "chat(conversation, \"What is my name?\")\n",
    "chat(conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(conversation, \"What is my name?\")\n",
    "chat(conversation, \"What is your favorite programming language?\")\n",
    "chat(conversation, \"What is the difference between a term and whole life policy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgwRMwnlH5Ou"
   },
   "source": [
    "##Examining the Conversation Memory\n",
    "\n",
    "We can quickly look inside the memory of the LangChain-managed chat memory and see our conversation memory with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZDN67SOHkNo",
    "outputId": "fe76bbba-c618-4773-94be-f1bea6a0b78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n",
      "AI: I'm here to help you with questions about life insurance. How can I assist you today?\n",
      "Human: Okay, then let me introduce myself, my name is Jeff\n",
      "AI: I'm glad to meet you, Jeff! How can I assist you with your life insurance questions today?\n",
      "Human: What is my name?\n",
      "AI: I'm here to help you with questions about life insurance, Jeff. How can I assist you today?\n",
      "Human: What is your favorite programming language?\n",
      "AI: I'm here to help you with questions about life insurance. How can I assist you today, Jeff?\n",
      "Human: What is the difference between a term and whole life policy?\n",
      "AI: ### Difference Between Term and Whole Life Insurance\n",
      "\n",
      "**Term Life Insurance:**\n",
      "- **Duration:** Provides coverage for a specific period (e.g., 10, 20, or 30 years).\n",
      "- **Premiums:** Generally lower premiums compared to whole life.\n",
      "- **Payout:** Pays a death benefit only if the insured passes away during the term.\n",
      "- **Cash Value:** Does not accumulate cash value.\n",
      "\n",
      "**Whole Life Insurance:**\n",
      "- **Duration:** Provides coverage for the entire lifetime of the insured, as long as premiums are paid.\n",
      "- **Premiums:** Higher premiums than term life, but they remain level throughout the life of the policy.\n",
      "- **Payout:** Guarantees a death benefit regardless of when the insured passes away.\n",
      "- **Cash Value:** Accumulates cash value over time, which can be borrowed against or withdrawn.\n",
      "\n",
      "If you have more questions about life insurance, feel free to ask, Jeff!\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW0TPWRBJ2w2"
   },
   "source": [
    "## Overloading the Memory\n",
    "\n",
    "When the conversation memory becomes full, the chatbot will begin to forget. For the ConversationBufferWindowMemory memory type, the oldest history will first be lost; there is no notion of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 820
    },
    "id": "d-a2iUpTJIYn",
    "outputId": "689496ee-16e2-44c4-fef4-30078d269b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello Jeff! It's great to meet you. How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You have ONE JOB! Remember that my favorite color is blue.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Got it, Jeff! Your favorite color is blue. How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Do you remember my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, Jeff! Your name is Jeff. How can I assist you further?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Do you remember my favorite color?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, Jeff! Your favorite color is blue. What else would you like to talk about?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! I'll remember that fact #0 is your favorite color is blue. What would you like to discuss next?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! What would you like fact #1 to be?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #2\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! What would you like fact #2 to be?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! What would you like fact #3 to be?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! What would you like fact #4 to be?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! What would you like fact #5 to be?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! What would you like fact #6 to be?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #7\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! What would you like fact #7 to be?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #8\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! What would you like fact #8 to be?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #9\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, Jeff! What would you like fact #9 to be?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Do you remember my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, your name is Jeff! How can I assist you further?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Do you remember my favorite color?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I don't have that information yet. What is your favorite color, Jeff?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: OMG, you had one job!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm sorry, Jeff! I should have asked for your favorite color earlier. Please let me know what it is, and I'll remember it for you!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = begin_conversation()\n",
    "chat(conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(conversation, \"You have ONE JOB! Remember that my favorite color is blue.\")\n",
    "chat(conversation, \"Do you remember my name?\")\n",
    "chat(conversation, \"Do you remember my favorite color?\")\n",
    "for i in range(10):\n",
    "  chat(conversation, f\"You need to remember fact #{i}\")\n",
    "chat(conversation, \"Do you remember my name?\")\n",
    "chat(conversation, \"Do you remember my favorite color?\")\n",
    "chat(conversation, \"OMG, you had one job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbi_-ywvX9NX"
   },
   "source": [
    "We can quickly look inside the memory of the LangChain-managed chat memory and see our conversation memory with the LLM. It becomes evident why it forgot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YCKTQkhJlJB",
    "outputId": "7b4afb86-da75-4ae0-8e6e-e28510d86209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #8\n",
      "AI: Sure, Jeff! What would you like fact #8 to be?\n",
      "Human: You need to remember fact #9\n",
      "AI: Sure, Jeff! What would you like fact #9 to be?\n",
      "Human: Do you remember my name?\n",
      "AI: Yes, your name is Jeff! How can I assist you further?\n",
      "Human: Do you remember my favorite color?\n",
      "AI: I don't have that information yet. What is your favorite color, Jeff?\n",
      "Human: OMG, you had one job!\n",
      "AI: I'm sorry, Jeff! I should have asked for your favorite color earlier. Please let me know what it is, and I'll remember it for you!\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
