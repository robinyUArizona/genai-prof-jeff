{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxu1Gfhx1pHg"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_10_4_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbNbAV281pHh"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 10: StreamLit**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwnvSYEQ1pHi"
   },
   "source": [
    "# Module 10 Material\n",
    "\n",
    "Module 10: StreamLit\n",
    "\n",
    "* Part 10.1: Running StreamLit in Google Colab [[Video]]() [[Notebook]](t81_559_class_10_1_streamlit.ipynb)\n",
    "* Part 10.2: StreamLit Introduction [[Video]]() [[Notebook]](t81_559_class_10_2_streamlit_intro.ipynb)\n",
    "* Part 10.3: Understanding Streamlit State [[Video]]() [[Notebook]](t81_559_class_10_3_streamlit_state.ipynb)\n",
    "* **Part 10.4: Creating a Chat Application** [[Video]]() [[Notebook]](t81_559_class_10_4_chat.ipynb)\n",
    "* Part 10.5: MultiModal Chat Application [[Video]]() [[Notebook]](t81_559_class_10_5_chat_multimodal.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLFov09h18yC"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWGARRT92DrA",
    "outputId": "0c8c3445-de95-41b1-af0b-72859b164a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai openai streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2MPPX0c1pHi"
   },
   "source": [
    "# Part 10.4: Creating a Chat Application\n",
    "\n",
    "In this module, we will guide you through the process of creating a StreamLit-based LLM chat application. To keep things accessible and straightforward, we'll run our app using Google Colab. Additionally, we'll introduce you to llm_util.py, a utility script designed to make it easier to work with LangChain-compatible large language models (LLMs). For this example, we'll be using OpenAI's LLM to power our chat application. By the end of this module, you'll have a functional, interactive chat app and a solid understanding of how to integrate LLMs into your projects using StreamLit.\n",
    "\n",
    "We will now create three files:\n",
    "\n",
    "* **app.py** - The main StreamLit chat application.\n",
    "* **llm_util.py** - The LLM utility that allows us to utilize any LangChain LLM for our chat application.\n",
    "* **llms.yaml** - A config file to define which LLM's we will use; for this example it will be OpenAI.\n",
    "\n",
    "## Chat Application\n",
    "\n",
    "This following code sets up a simple chatbot application using the OpenAI API and Streamlit, a Python library that allows for easy creation of web apps. The script, named app.py, begins by importing various modules necessary for its functionality. It uses ```openai.OpenAI``` to interact with the OpenAI language model, streamlit to create the user interface, and sys for handling command-line arguments. Additionally, it imports custom utilities from ```llm_util```, along with specific components from the LangChain library, including ```ConversationSummaryMemory```, ```PromptTemplate```, and ```ConversationChain```. These tools are crucial for managing the conversation's context, creating input templates, and facilitating conversational interactions.\n",
    "\n",
    "The script starts by checking if the required command-line arguments are provided. It expects a language model profile as an argument; if this is not specified, it halts execution and prompts the user to provide the necessary input. This mechanism ensures that the appropriate language model is selected before launching the chatbot.\n",
    "\n",
    "The heart of the chatbot setup lies in the create_chatbot function. This function establishes the chatbot using the specified language model and incorporates a memory component, ```ConversationSummaryMemory```, which keeps track of the ongoing conversation's context. By utilizing this memory, the chatbot can generate more coherent and contextually relevant responses over time. Additionally, the function defines a simple template using LangChain's PromptTemplate, formatting how the conversation history and user input are combined. It then returns a ConversationChain object that processes these conversations, ensuring that responses are influenced by the entire chat history.\n",
    "\n",
    "To create an interactive interface, the script uses Streamlit. It begins by setting a title, \"Chat,\" for the application. To manage the chatbotâ€™s state and keep track of the conversation, the script employs Streamlit's session state mechanism. It checks if a chatbot instance (st.session_state.chat) already exists; if not, it initializes a new one with the LLM profile provided via the command line. Similarly, it sets up a list to store the conversation messages (st.session_state.messages) if it does not already exist.\n",
    "\n",
    "The conversation history is then iterated through and displayed using Streamlit's chat-specific functions. Each message, whether from the user or the assistant, is rendered in the chat interface. This provides a running log of the conversation, giving the user an ongoing view of their interaction with the chatbot.\n",
    "\n",
    "User interaction occurs through a chat input box facilitated by st.chat_input. When the user provides input, the script appends the message to the conversation log and displays it in the interface. The chatbot then processes this input using the predict method of ConversationChain, generating a response based on the conversation's context. This response is displayed in the chat window and added to the session state, ensuring the entire dialogue is stored and utilized for subsequent interactions.\n",
    "\n",
    "In essence, this script leverages LangChain's memory management and templating capabilities to create a dynamic chatbot, while Streamlit provides a user-friendly interface for real-time interaction. The combination of these tools allows the chatbot to maintain context throughout the conversation, resulting in more meaningful and coherent exchanges. Furthermore, by requiring a language model profile as a command-line argument, the script is flexible and adaptable to various LLM configurations, allowing the user to customize the chatbot's behavior depending on their specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEQ9c5Akqj_R",
    "outputId": "8c5eeabd-f406-4c8f-f330-52ed4e285985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from openai import OpenAI\n",
    "import streamlit as st\n",
    "from llm_util import *\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "import sys\n",
    "\n",
    "# This retrieves all command line arguments as a list\n",
    "arguments = sys.argv\n",
    "if len(sys.argv) != 2:\n",
    "    print(\"Please specify the llm to use as the first argument\")\n",
    "    st.stop()\n",
    "else:\n",
    "    profile = sys.argv[1]\n",
    "\n",
    "\n",
    "def create_chatbot(llm):\n",
    "    memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "    template = \"\"\"{history}\\n{input}\\n\\n\n",
    "    \"\"\"\n",
    "    PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "    return ConversationChain(llm=llm, prompt=PROMPT, memory=memory, verbose=False)\n",
    "\n",
    "\n",
    "st.title(\"Chat\")\n",
    "\n",
    "if \"chat\" not in st.session_state:\n",
    "    client = open_llm(profile)\n",
    "    st.session_state.chat = create_chatbot(client)\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(\n",
    "            message[\"content\"],\n",
    "        )\n",
    "\n",
    "if prompt := st.chat_input(\"What is up?\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(\n",
    "            prompt,\n",
    "        )\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response = st.session_state.chat.predict(input=prompt)\n",
    "        st.markdown(\n",
    "            response,\n",
    "        )\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSKyHUHqhCcx"
   },
   "source": [
    "## LLM Utility\n",
    "\n",
    "\n",
    "This following code enables the chat application to use various language models supported by LangChain based on a configuration file. The llm_util.py script serves as a utility that dynamically loads and initializes different language models using configurations specified in a YAML file (llms.yaml). This approach provides a flexible way to change the language model without modifying the main application code, allowing for easy experimentation and customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXml9nrGqj_R",
    "outputId": "5400a880-69fd-4b87-b11a-32e8132e06ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llm_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile llm_util.py\n",
    "import yaml\n",
    "\n",
    "# Load the YAML file\n",
    "def load_yaml(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "# Function to dynamically import a class based on a string path (e.g., \"langchain_community.chat_models.ChatOllama\")\n",
    "def get_class(class_path):\n",
    "    module_path, class_name = class_path.rsplit(\".\", 1)\n",
    "    module = __import__(module_path, fromlist=[class_name])\n",
    "    return getattr(module, class_name)\n",
    "\n",
    "\n",
    "# Open Language Model Server function\n",
    "def open_llm(server_name):\n",
    "    config = load_yaml(\"llms.yaml\")\n",
    "    for server in config[\"servers\"]:\n",
    "        if server[\"name\"] == server_name:\n",
    "            class_path = server[\"class\"]\n",
    "            clazz = get_class(class_path)\n",
    "            # Remove 'class' and 'name' from the parameters as they're not needed for initialization\n",
    "            params = {k: v for k, v in server.items() if k not in [\"class\", \"name\"]}\n",
    "\n",
    "            return clazz(**params)\n",
    "    raise ValueError(f\"Server '{server_name}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wk6T0aJEqj_S",
    "outputId": "207c08e7-ee31-4752-e632-087d9cdccc40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llms.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile llms.yaml\n",
    "\n",
    "servers:\n",
    "  - name: server1\n",
    "    class: langchain_openai.ChatOpenAI\n",
    "    model: gpt-4o-mini\n",
    "    temperature: 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMqVgvmxhySA"
   },
   "source": [
    "Next, we obtain the password for our StreamLit server we are about to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iJmAPR8c-JE",
    "outputId": "bfb03037-7bf6-45ef-bfb1-8315823f15c9"
   },
   "outputs": [],
   "source": [
    "# !curl https://loca.lt/mytunnelpassword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3lKrMcvh3_F"
   },
   "source": [
    "We launch the StreamLit server and obtain its URL. You will need the above password when you access the URL it gives you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSLPrByztQOe",
    "outputId": "1eebbf3a-c3bf-4205-abf0-87c53f5397aa"
   },
   "outputs": [],
   "source": [
    "# !streamlit run app.py server1 &>/content/logs.txt &\n",
    "# !npx --yes localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXvoDfPzkgwc"
   },
   "source": [
    "The following config file shows how to utilize other LLM servers:\n",
    "\n",
    "* server1 - Ollama\n",
    "* server2 - OpenAI\n",
    "* server3 - Bedrock (AWS)\n",
    "* server4 - LMStudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLPuHtYQqj_S"
   },
   "source": [
    "```\n",
    "servers:\n",
    "  - name: server1\n",
    "    class: langchain_community.chat_models.ChatOllama\n",
    "    base_url: http://localhost:11434\n",
    "    model: llama2\n",
    "    temperature: 0\n",
    "  - name: server2\n",
    "    class: langchain_openai.ChatOpenAI\n",
    "    model: gpt-4o-mini\n",
    "    temperature: 0\n",
    "  - name: server3\n",
    "    class: langchain_aws.ChatBedrock\n",
    "    model_id: amazon.titan-text-express-v1\n",
    "    model_kwargs:\n",
    "      temperature: 0.1\n",
    "  - name: server4\n",
    "    class: langchain_openai.ChatOpenAI\n",
    "    base_url: http://localhost:1234/v1/\n",
    "    model: TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
    "    openai_api_key: None\n",
    "    temperature: 0\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUH52Q_wknk0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
