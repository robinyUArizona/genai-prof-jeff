{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_06_3_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 6: Retrieval-Augmented Generation (RAG)**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 6 Material\n",
    "\n",
    "* Part 6.1: Introduction to Retrieval-Augmented Generation (RAG) [[Video]](https://www.youtube.com/watch?v=qA52K0K181Q) [[Notebook]](t81_559_class_06_1_rag.ipydb)\n",
    "* Part 6.2: Introduction to ChromaDB [[Video]](https://www.youtube.com/watch?v=R53lo4sevLQ) [[Notebook]](t81_559_class_06_2_chromadb.ipynb)\n",
    "* **Part 6.3: Understanding Embeddings** [[Video]](https://www.youtube.com/watch?v=Tq82Gl2ZZNM) [[Notebook]](t81_559_class_06_3_embeddings.ipynb)\n",
    "* Part 6.4: Question Answering Over Documents [[Video]](https://www.youtube.com/watch?v=hCwL_lW-gP0) [[Notebook]](t81_559_class_06_4_qa.ipynb)\n",
    "* Part 6.5: Embedding Databases [[Video]](https://www.youtube.com/watch?v=BG2gT4uYxhM) [[Notebook]](t81_559_class_06_5_embed_db.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "24f19116-7ed8-44a0-f4ab-e0b09fd5b365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai pypdf chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 6.3: Understanding Embeddings\n",
    "\n",
    "An [embedding](https://platform.openai.com/docs/guides/embeddings) is a vector (list) of floating point numbers. The [distance](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use) between two vectors measures their relatedness. Small distances suggest high relatedness, and large distances suggest low relatedness.\n",
    "\n",
    "The two embedding models that you will choose between in OpenAI are as follows:\n",
    "\n",
    "* text-embedding-3-small\n",
    "* text-embedding-3-large\n",
    "\n",
    "Choosing between the two OpenAI embedding models, \"text-embedding-3-small\" and \"text-embedding-3-large,\" depends on several factors related to your specific use case, including performance requirements, computational resources, and the nature of the tasks you need the embeddings for. Here are some key considerations:\n",
    "\n",
    "1. **Performance and Accuracy**:\n",
    " * **text-embedding-3-large**: Generally, larger models tend to capture more nuanced and complex relationships within the text, leading to better performance in tasks that require a deep understanding of language, such as semantic similarity, sentiment analysis, and more sophisticated NLP tasks.\n",
    " * **text-embedding-3-small**: Smaller models may not be as accurate or detailed as larger ones, but they can still perform well on many tasks, especially those with less complexity or when fine-tuned on specific datasets.\n",
    "2. **Computational Resources:**\n",
    " * **text-embedding-3-large**: Requires more computational power and memory. This requirement is important if you are deploying the model in a resource-constrained environment or need to process a large volume of data in real time.\n",
    " * **text-embedding-3-small**: More efficient resource usage, making it a better choice for applications with limited computational power or when operating at a scale where cost and speed are critical factors.\n",
    "3. **Latency and Throughput:**\n",
    " * **text-embedding-3-large**: Typically, larger models have higher latency due to their complexity, which might impact real-time applications.\n",
    " * **text-embedding-3-small**: Lower latency and faster inference times benefit applications requiring quick responses.\n",
    "4. **Cost:**\n",
    " * **text-embedding-3-large**: Likely to incur higher operational costs due to greater computational requirements.\n",
    " * **text-embedding-3-small**: More cost-effective, particularly for large-scale deployments.\n",
    " Use Case Specifics:\n",
    "\n",
    "The large model might be more appropriate for applications needing high precision and where detailed semantic understanding is crucial, such as nuanced text analysis or advanced AI research.\n",
    "The small model could be the better choice for applications where speed, cost, and efficiency are more critical, such as real-time systems, chatbots, or applications with more straightforward text processing needs.\n",
    "\n",
    "In summary,\n",
    "\n",
    "* Choose text-embedding-3-large if:\n",
    " * You need high accuracy and detailed semantic understanding.\n",
    " * You have sufficient computational resources and budget.\n",
    " * Latency is not a critical concern.\n",
    "* Choose text-embedding-3-small if:\n",
    " * You require efficient resource usage and lower costs.\n",
    " * You need faster inference times.\n",
    "\n",
    "The tasks are less complex, or the environment needs more resource-constrained.\n",
    "Evaluating your requirements and constraints will help you decide which model to use.\n",
    "\n",
    "## Instantiating an Embedding Model\n",
    "\n",
    "For this class, I suggest that you use **text-embedding-3-small**. It has all the capabilities that we need and will stretch your credits further. Let's begin by creating a client that utilizes this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wP9LYwNUSKyv"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGs1Fqk0Vu-6"
   },
   "source": [
    "Let's start by understanding the basics of an embedding model and vectors. An embedding model is a tool that can convert any text string, regardless of its length, into a vector. This vector is a unique representation of the text string. If the vectors for two text strings are the same, it means the text strings are identical. If the vectors are different, it suggests that the text strings are distinct. However, this difference is not a simple comparison. Even two very different text strings, with the same meaning, can produce similar vectors.\n",
    "\n",
    "To begin with, let's look at vectors for individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iq3WUSr7T1QK",
    "outputId": "fec4ed4c-f518-419a-ca1b-80995846fdb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "l1 = embeddings_model.embed_query(\"dog\")\n",
    "l2 = embeddings_model.embed_query(\"Something that is a bit longer than a word.\")\n",
    "\n",
    "print(type(l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMqiJ6zFZUue"
   },
   "source": [
    "As you can see, the output is just a regular Python list. These lists are quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNt_opHMX1B7",
    "outputId": "c0c21cc3-b5a0-4909-df01-b17351f4697a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "print(len(l1))\n",
    "print(len(l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrwZGIA-ZbK_"
   },
   "source": [
    "The length of this string will remain consistent across queries of the same model. The larger version of the model, while it does a better job of creating vectors that differentiate strings, doesn't always require a larger vector length to achieve this increase in quality. It's a nuanced concept worth exploring further.\n",
    "\n",
    "If we display the actual list itself, we can see that it is just a collection of numbers. Here, we display only the first ten elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MePSP-eGZpWA",
    "outputId": "e9a1e208-4f3c-4555-a3bb-fc99582c20ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05113774910569191, -0.01870863139629364, -0.004298428073525429, 0.07271610200405121, -0.007174310740083456, -0.014693480916321278, -0.0059395902790129185, 0.005037412978708744, 0.018954960629343987, -0.01090618409216404]\n"
     ]
    }
   ],
   "source": [
    "print(l1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtbM9gKCaqxU"
   },
   "source": [
    "## Comparing Vectors\n",
    "\n",
    "To compare these vectors we will use the mathematical capabilities of [Numpy](https://numpy.org/). There are multiple different approaches to compare vectors in mathematics, some of the most common are presented here.\n",
    "\n",
    "* **Dot Product**: Measures the cosine of the angle between two vectors, indicating their directional similarity. Used in determining orthogonality, projection, and in various applications like computer graphics and machine learning.\n",
    "\n",
    "* **Cross Product**: Computes a vector perpendicular to two given vectors in three-dimensional space, useful for finding the normal vector to a plane and in physics for torque and angular momentum calculations.\n",
    "\n",
    "* **Euclidean Distance**: Calculates the straight-line distance between two vectors, widely used in machine learning for clustering and nearest neighbor algorithms.\n",
    "\n",
    "* **Cosine Similarity**: Evaluates the cosine of the angle between two vectors, emphasizing the orientation rather than the magnitude, often used in text mining and information retrieval to compare document similarity.\n",
    "\n",
    "* **Manhattan Distance**: Measures the sum of absolute differences between the components of two vectors, useful in grid-based pathfinding algorithms and some machine learning applications.\n",
    "\n",
    "OpenAI [suggests](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) that we use Cosine Similarity to compare their vectors, because it perserves the magnitude. The sign of the individual vector numbers is important, we do not want to discard it.\n",
    "\n",
    "We will begin by converting the two embeddings from Python lists to Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xgKMSKraapv3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "vec1 = np.array(l1)\n",
    "vec2 = np.array(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVyEFevXgCqR"
   },
   "source": [
    "OpenAI specifies that all their embeddings are normalized to length 1, often called [unit vectors](https://en.wikipedia.org/wiki/Unit_vector). This fact means that:\n",
    "\n",
    "* Cosine similarity can be computed slightly faster using just a dot product\n",
    "* Cosine similarity and Euclidean distance will result in the identical rankings\n",
    "\n",
    "Now, let's put this into practice. To verify that these embeddings are indeed of length 1, we can use a handy function. This function is specifically designed to analyze the length of a vector, making it a useful tool in our exploration of OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0GPYq2wodpyJ"
   },
   "outputs": [],
   "source": [
    "def analyze_vector_length(vector):\n",
    "  # Calculate the length (norm) of the vector\n",
    "  length = np.linalg.norm(vector)\n",
    "\n",
    "  # Check if the vector is a unit vector\n",
    "  is_unit_vector = np.isclose(length, 1.0)\n",
    "\n",
    "  print(f\"Vector: {vector}\")\n",
    "  print(f\"Length of the vector: {length}\")\n",
    "  print(f\"Is the vector a unit vector? {'Yes' if is_unit_vector else 'No'}\")\n",
    "\n",
    "  # Normalize the vector to make it a unit vector\n",
    "  unit_vector = vector / length\n",
    "\n",
    "  # Verify the length of the normalized vector\n",
    "  normalized_length = np.linalg.norm(unit_vector)\n",
    "\n",
    "  print(f\"Normalized vector (unit vector): {unit_vector}\")\n",
    "  print(f\"Length of the normalized vector: {normalized_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRrx2dZhfrfG",
    "outputId": "f01bf3e7-0232-420e-942a-7ea191b45fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector: [ 0.05113775 -0.01870863 -0.00429843 ...  0.02879578  0.00215999\n",
      "  0.01790806]\n",
      "Length of the vector: 0.9999999997385218\n",
      "Is the vector a unit vector? Yes\n",
      "Normalized vector (unit vector): [ 0.05113775 -0.01870863 -0.00429843 ...  0.02879578  0.00215999\n",
      "  0.01790806]\n",
      "Length of the normalized vector: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "analyze_vector_length(vec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V459AdOAfqmd"
   },
   "source": [
    "Next, let's calculate the actual cosine similarity between our two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2fgizuzXPAT",
    "outputId": "c6bbbd9d-7a90-403f-95be-4173bc206828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.16733579901891651\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "vec1 = np.array(l1)\n",
    "vec2 = np.array(l2)\n",
    "\n",
    "# Calculate the dot product\n",
    "dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "# Calculate the magnitudes (L2 norms)\n",
    "magnitude_vec1 = np.linalg.norm(vec1)\n",
    "magnitude_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "\n",
    "print(f\"Cosine similarity: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO3zv6WGhDEw"
   },
   "source": [
    "The denominator above is 1.0, as shown here. This is due to the unit vector property. We can simplify our unit vector comparison to just the dot product, as indicated by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMKiwXR7hGl1",
    "outputId": "546a02e8-55f4-4ad7-b9d0-d037a78354be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000168870586\n"
     ]
    }
   ],
   "source": [
    "print(magnitude_vec1 * magnitude_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1weLEmSEhfvY"
   },
   "source": [
    "The dot product can be calculated as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c981Unthh5h",
    "outputId": "75df5174-178e-4072-b8c8-00f8fdcb1597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16733580184472596\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(vec1, vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzGVjgjbkeOC"
   },
   "source": [
    "## Evaluating Similarities of Strings\n",
    "\n",
    "We will begin by creating a simple function to compare two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ooX8bTmFlFRJ"
   },
   "outputs": [],
   "source": [
    "def compare_str(embeddings_model, text1, text2):\n",
    "    \"\"\"\n",
    "    This function returns the dot product of embeddings for two given text strings.\n",
    "\n",
    "    Parameters:\n",
    "    embeddings_model: The embeddings model to use for generating embeddings.\n",
    "    text1 (str): The first text string.\n",
    "    text2 (str): The second text string.\n",
    "\n",
    "    Returns:\n",
    "    float: The dot product of the embeddings for text1 and text2.\n",
    "    \"\"\"\n",
    "    # Get the embeddings for the two text strings\n",
    "    embedding1 = embeddings_model.embed_query(text1)\n",
    "    embedding2 = embeddings_model.embed_query(text2)\n",
    "\n",
    "    # Convert embeddings to numpy arrays for dot product calculation\n",
    "    embedding1_array = np.array(embedding1)\n",
    "    embedding2_array = np.array(embedding2)\n",
    "\n",
    "    # Calculate and return the dot product\n",
    "    dot_product = np.dot(embedding1_array, embedding2_array)\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20QUKA9-qa4J"
   },
   "source": [
    "Lets try it with two descriptions of a \"lawn mower\" that do not use many similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dgEmHV2lHEs",
    "outputId": "6553350e-fb4b-48e0-e8cf-6dec3fa43657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6298291212454291"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_str(embeddings_model,\n",
    "            \"A machine that helps people to cut grass.\",\n",
    "            \"Device with blades to cut plants under it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etJISf2MmcY1"
   },
   "source": [
    "The value of the dot product (cosine similarity) ranges from -1 to 1. We can interpret it as follows:\n",
    "\n",
    "* High Similarity: Values close to 1 indicate high similarity.\n",
    "* Low Similarity: Values close to -1 indicate high dissimilarity.\n",
    "* Neutral/No Similarity: Values close to 0 indicate no apparent similarity.\n",
    "\n",
    "So a value of 0.62 means they are reasonably similar. Lets adjust it to compare a lawn mower to an airplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b43n_JgOlWD1",
    "outputId": "429def1f-dbaf-43c8-8b66-c819319451be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2694946993627674"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_str(embeddings_model,\n",
    "            \"A machine that helps people to cut grass.\",\n",
    "            \"Vehicle that flys through the air.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBucz-M7nxp3"
   },
   "source": [
    "We can see the similarity is lower."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
