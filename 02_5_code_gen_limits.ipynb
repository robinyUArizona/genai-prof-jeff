{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_02_5_code_gen_limits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 2: Code Generation**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 2 Material\n",
    "\n",
    "* Part 2.1: Prompting for Code Generation [[Video]](https://www.youtube.com/watch?v=HVId6kYKKgQ) [[Notebook]](t81_559_class_02_1_dev.ipynb)\n",
    "* Part 2.2: Handling Revision Prompts [[Video]](https://www.youtube.com/watch?v=APpV46tplXA) [[Notebook]](t81_559_class_02_2_multi_prompt.ipynb)\n",
    "* Part 2.3: Using a LLM to Help Debug [[Video]](https://www.youtube.com/watch?v=VPqSNb38QK0) [[Notebook]](t81_559_class_02_3_llm_debug.ipynb)\n",
    "* Part 2.4: Tracking Prompts in Software Development [[Video]](https://www.youtube.com/watch?v=oUFUuYfvXZU) [[Notebook]](t81_559_class_02_4_software_eng.ipynb)\n",
    "* **Part 2.5: Limits of LLM Code Generation** [[Video]](https://www.youtube.com/watch?v=dKtRI0LZSyY) [[Notebook]](t81_559_class_02_5_code_gen_limits.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "4c950075-215a-464b-f04b-7a831d498767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.4-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Downloading langchain_core-0.1.46-py3-none-any.whl (299 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.51-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
      "Collecting openai<2.0.0,>=1.10.0 (from langchain_openai)\n",
      "  Downloading openai-1.23.6-py3-none-any.whl (311 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m939.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain_openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.2.1)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, h11, typing-inspect, tiktoken, marshmallow, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-text-splitters, langchain_openai, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed dataclasses-json-0.6.4 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.46 langchain-text-splitters-0.0.1 langchain_openai-0.1.4 langsmith-0.1.51 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.23.6 orjson-3.10.1 packaging-23.2 tiktoken-0.6.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 2.5: Limits of LLM Code Generation\n",
    "\n",
    "While LLMs can significantly boost productivity in software development, it's equally important to understand their limitations, particularly in code generation. This section will conduct a thorough examination of these limitations, leaving no stone unturned. The following list succinctly outlines the limitations we will discuss.\n",
    "\n",
    "* Analysis of large amounts of data\n",
    "* Operating at the boundary of human understanding\n",
    "* Iterative improvement\n",
    "* Dealing with breaking changes in new libraries\n",
    "* Coping with new features of programming languages\n",
    "* Large proprietary function libraries\n",
    "* Modifying large monolithic software projects\n",
    "\n",
    "In the following sections, we will examine each of these.\n",
    "\n",
    "## Analysis of Large Amounts of Data\n",
    "\n",
    "Large Language Models (LLMs) like GPT are primarily designed to generate human-like text based on patterns learned from a diverse range of sources. However, they face significant limitations when tasked with writing code that must handle large datasets, especially when the specifics of the data have not been previously analyzed. This is primarily due to several reasons:\n",
    "\n",
    "* **Insufficient Understanding of Data Characteristics:** LLMs generate responses based on the training data they have been exposed to, rather than having an intrinsic understanding of new data. They lack the ability to analyze and draw insights from data directly, especially if the data is large and complex, making them unsuitable for tasks that require deep data analysis to determine patterns, anomalies, or statistical properties.\n",
    "\n",
    "* **Memory and Processing Constraints:** Despite their advanced capabilities, LLMs operate with limited context windows and cannot maintain state across long interactions. This makes it challenging to deal with large volumes of data where understanding the complete context or maintaining an ongoing analysis is necessary. They cannot, for example, dynamically load, process, or analyze gigabytes of data in real-time or in the sequence it may be required by the application.\n",
    "\n",
    "Handling data where both the quality and structure are unknown further complicates the use of LLMs for code generation:\n",
    "\n",
    "* **Unknown Data Quality:** LLMs are not equipped to assess or ensure the quality of the data they are dealing with. In scenarios where the data quality is poor (e.g., significant amounts of missing values, incorrect entries), a model may generate code that does not account for these issues, potentially leading to inaccurate outputs or failures in data processing tasks.\n",
    "\n",
    "* **Unstructured Data:** LLMs are adept at generating code for well-defined problems with structured data (like SQL queries for database manipulation). However, when the data structure is unknown or the data is unstructured (like raw text, images, or irregular time-series), LLMs struggle to generate effective code. They are not designed to infer data structure or to create data schemas without explicit instructions or examples. Consequently, the code generated may be inefficient or inadequate for processing the data correctly.\n",
    "\n",
    "To mitigate these limitations, it's advisable to combine the capabilities of LLMs with traditional data analysis techniques and tools:\n",
    "\n",
    "* **Pre-processing and Data Analysis:** Before employing an LLM for code generation, conduct a thorough analysis and pre-processing of the data using conventional data analysis tools. This approach helps in understanding the structure, quality, and key characteristics of the data, which can then be used to provide detailed prompts to the LLM, guiding it to generate more accurate and effective code.\n",
    "* **Hybrid Approaches:** Integrating LLMs with specialized data processing libraries and frameworks can enhance their utility. For example, using an LLM to draft initial code frameworks based on user queries and then refining these outputs with expert systems or automated data analysis tools can yield better results.\n",
    "\n",
    "* **Continuous Learning and Adaptation:** Incorporating feedback loops where the LLM's output is continuously evaluated and refined can help in adapting the generated code to better handle large and complex datasets over time.\n",
    "By understanding these limitations and strategically planning the use of LLMs in conjunction with other data analysis methodologies, developers can leverage the strengths of generative AI while minimizing its weaknesses in real-world applications.\n",
    "\n",
    "## Operating at the Boundary of Human Understanding\n",
    "\n",
    "When it comes to generating code that lies at the edge of current human understanding, such as developing new machine learning optimization methods or innovative neural network architectures, Large Language Models (LLMs) like GPT face distinct challenges:\n",
    "\n",
    "* **Lack of Novelty in Outputs:** LLMs generate responses based on patterns and information present in their training data. They excel at interpolating from existing knowledge but are fundamentally limited in their ability to create truly novel theories or methods that have not been previously conceptualized. This is particularly relevant in fields like machine learning, where innovation often requires stepping beyond the established boundaries of current understanding.\n",
    "\n",
    "* **Complexity and Depth of Knowledge Required:** Advanced fields such as neural network design and optimization algorithms involve deep technical knowledge and a complex interplay of various mathematical and computational principles. LLMs, although proficient in handling a wide range of topics at a surface level, typically do not possess the depth of understanding required to autonomously develop groundbreaking new methods that are both functional and optimal.\n",
    "\n",
    "The question of whether an LLM like ChatGPT can enhance its own program code to boost its capabilities brings up significant limitations:\n",
    "\n",
    "* **Self-awareness and Meta-programming Limits:** LLMs do not have self-awareness or the ability to comprehend their own underlying architecture in a meaningful way. While they can assist in writing code by following patterns seen in their training data, they lack the capability to introspectively analyze and optimize their own codebase. This is a significant barrier to self-improvement, which requires not only understanding the code but also the complex interactions within the code that contribute to overall performance.\n",
    "* **Ethical and Safety Considerations:** Even if technically feasible, allowing an LLM to modify its own code raises serious ethical and safety concerns. Autonomous self-modification could lead to unintended behaviors or decisions that deviate from designed safety guidelines and ethical norms. This risk is particularly acute in AI development, where robustness and predictability are paramount.\n",
    "\n",
    "To address these limitations, researchers and developers can employ several strategies:\n",
    "\n",
    "* **Human-AI Collaboration:** Instead of relying solely on LLMs to pioneer new concepts or improve themselves, a collaborative approach where human expertise guides and refines the output of LLMs proves more effective. Humans can provide creative insights, critical thinking, and ethical oversight that LLMs lack.\n",
    "* **Incorporating External Knowledge and Tools:** Integrating LLMs with cutting-edge research tools and databases can help bridge the gap between existing knowledge and innovative output. By using LLMs to synthesize and iterate on the latest research findings, the generation of novel ideas can be accelerated.\n",
    "\n",
    "* **Iterative Development and Testing:** Employing an iterative approach to development, where outputs from LLMs are continuously tested, evaluated, and refined, can gradually improve the quality and innovativeness of generated code. This process also ensures alignment with current scientific standards and safety requirements.\n",
    "\n",
    "While LLMs represent a powerful tool for numerous applications, their utility in developing frontier technologies and self-optimization is limited by their reliance on existing knowledge and lack of deeper cognitive abilities. Recognizing these limitations is crucial for effectively leveraging their strengths while mitigating risks.\n",
    "\n",
    "## Iterative Improvement\n",
    "\n",
    "Large Language Models (LLMs) like GPT are invaluable tools for generating initial codebases or drafts, but they encounter specific challenges when the task involves iterative improvements, such as refining the accuracy of machine learning models through multiple cycles of development:\n",
    "\n",
    "* **Lack of State Persistence:** One of the fundamental limitations of LLMs in iterative processes is their lack of state persistence. Each interaction with an LLM is stateless, meaning it does not remember previous interactions. This trait makes it inherently difficult for LLMs to engage in tasks that require memory of past iterations, adjustments, and refinements, which are crucial for incremental code development and model tuning.\n",
    "* **Difficulty in Evaluating Code Efficacy:** LLMs do not inherently evaluate the effectiveness or efficiency of the code they generate. While they can produce syntactically correct and logically sound code, they are unable to judge its performance or suggest improvements based on execution results. This is particularly limiting in scenarios where code needs to be optimized based on its runtime behavior or outcome, such as in data preprocessing or model training.\n",
    "\n",
    "Hyperparameter tuning is critical in optimizing machine learning models, but LLMs face particular challenges in this area:\n",
    "\n",
    "* **Lack of Direct Interaction with the Execution Environment:** LLMs can suggest hyperparameters based on patterns seen during training, but they do not have the capability to directly interact with the model training environment or dynamically adjust hyperparameters based on real-time feedback. This limits their utility in scenarios where hyperparameter tuning is highly dynamic and dependent on ongoing results.\n",
    "* **Generic Suggestions Without Contextual Awareness:** The suggestions provided by LLMs for hyperparameters are generally based on common practices and broad guidelines rather than the specific nuances of the data or model architecture in use. This can lead to suboptimal performance where fine-tuning based on specific dataset characteristics and goals would have been more beneficial.\n",
    "\n",
    "Understanding the distinction between LLM-driven code generation and AutoML systems clarifies the limitations and appropriate applications for each:\n",
    "\n",
    "* **Scope of Functionality:** LLMs primarily generate code based on textual prompts and are not designed to automatically execute or refine this code. In contrast, AutoML platforms are built specifically to automate the end-to-end process of model development, including feature selection, model choice, and hyperparameter tuning. AutoML systems are equipped with mechanisms to not only generate but also evaluate and refine models iteratively.\n",
    "* **Performance Evaluation:** Unlike LLMs, AutoML systems actively evaluate the performance of various model configurations, making them inherently suitable for tasks that require experimentation and iterative improvement. AutoML systems use this performance data to make informed decisions about the best models and configurations to deploy.\n",
    "\n",
    "To navigate these limitations, integrating LLMs with other tools and platforms can be effective:\n",
    "\n",
    "* **Combining LLMs with AutoML:** Using LLMs to generate initial code or to explain and document the steps of an AutoML process can harness the strengths of both. While AutoML handles the iterative and evaluative aspects, LLMs can assist in setting up the frameworks or explaining the complex decisions and results in an understandable manner.\n",
    "\n",
    "* **Supervised Iterations:** Incorporating human oversight in iterative cycles where LLMs are used can help in adjusting the outputs based on performance evaluations. This approach allows for leveraging the rapid code generation capabilities of LLMs while ensuring that the iterative improvements are guided by practical performance insights.\n",
    "\n",
    "Despite their advanced capabilities, LLMs have inherent limitations in scenarios requiring iterative improvement and dynamic tuning, areas where systems like AutoML excel. Understanding these differences is crucial for effectively applying these technologies in real-world scenarios.\n",
    "\n",
    "## Dealing with Breaking Changes in New Library Versions\n",
    "\n",
    "Large Language Models (LLMs) like GPT are adept at generating code based on a wide array of programming knowledge, but they encounter specific challenges when dealing with breaking changes in new versions of software libraries:\n",
    "\n",
    "* **Lack of Real-time Updates:** LLMs are trained on a fixed dataset that does not automatically update to reflect the latest versions of libraries or frameworks. This means they might not be aware of recent changes, deprecations, or new features until their training data is updated. As a result, they may generate code that is incompatible with the latest versions of libraries, leading to errors or deprecated function calls.\n",
    "* **Inability to Interpret Release Notes:** Understanding the nuances of library updates often requires interpreting release notes and documentation that describe changes, improvements, and removed features. LLMs can synthesize information from text but do not inherently understand the implications of these updates on existing codebases, limiting their ability to advise on or automatically refactor code to address breaking changes.\n",
    "\n",
    "Software libraries evolve, and new versions can introduce changes that are not backward-compatible, known as \"breaking changes.\" LLMs face significant limitations in adapting generated code to these changes:\n",
    "\n",
    "* **Fixed Knowledge Base:** The knowledge of LLMs is essentially frozen at the point of their last training update, meaning they lack awareness of any developments or conventions that have emerged since. This time lag can result in recommendations that are out of sync with current best practices or API requirements.\n",
    "* **Generic Coding Patterns:** While LLMs generate code based on general patterns they have learned, they may not account for specific context or requirements introduced in new library versions. This can lead to code that, while functionally correct in a previous version, fails to comply with new APIs or utilizes deprecated features.\n",
    "\n",
    "To mitigate these limitations and effectively use LLMs in environments where library versions change frequently, several approaches can be considered:\n",
    "\n",
    "* **Manual Review and Testing:** It is crucial to manually review and test code generated by LLMs, especially when working with libraries known to undergo frequent updates. This helps ensure that the code not only functions correctly but also adheres to the latest standards and practices.\n",
    "\n",
    "* **Integration with Continuous Integration Tools:** Automating the testing of generated code through continuous integration (CI) tools can help identify and resolve issues arising from library updates. CI tools can automatically test the compatibility of new code with various library versions, providing feedback that can be used to refine the LLM’s outputs.\n",
    "\n",
    "* **Use of Version-Specific Prompts:** When generating code, providing LLMs with version-specific prompts can help tailor the output to comply with the particularities of that version. For example, specifying the version number of a library in the prompt can guide the LLM to generate more accurate and compatible code snippets.\n",
    "\n",
    "* **Regular Model Updates and Training:** Periodically updating the training data of LLMs to include the latest documentation, release notes, and coding standards can help minimize the knowledge gap and enhance the relevance of the code they generate.\n",
    "\n",
    "While LLMs are powerful tools for code generation, their effectiveness can be compromised by rapid changes in software libraries. Understanding and addressing these limitations is key to leveraging LLMs effectively in software development workflows, particularly in dynamic environments where keeping pace with technology advancements is crucial.\n",
    "\n",
    "## Dealing with New Libraries\n",
    "\n",
    "Large Language Models (LLMs) such as GPT are instrumental in generating code based on existing and well-understood programming languages and libraries. However, they face significant hurdles when it comes to dealing with libraries that are newly released:\n",
    "\n",
    "* **Lack of Training Data on New Libraries:** Since LLMs rely heavily on the data they were trained on, their ability to generate code using newly released libraries is limited by the absence of these libraries in their training datasets. This lack of familiarity means that LLMs may not produce effective or accurate code snippets that leverage the new features or syntax introduced in these libraries.\n",
    "\n",
    "* **Unawareness of Library Documentation and Best Practices:** New libraries come with their own documentation, coding examples, and best practices, which are not immediately known to LLMs until these elements become part of their training data. Consequently, LLMs may struggle to provide guidance on the correct usage of these libraries or to adopt the most effective coding practices associated with them.\n",
    "\n",
    "The introduction of new libraries often includes innovative features that may not be immediately understandable by LLMs due to their advanced nature or unique implementation:\n",
    "\n",
    "* **Inability to Conceptualize New Paradigms:** New libraries can introduce shifts in programming paradigms or offer unique approaches to solving problems. LLMs, trained on past data, might not only lack knowledge of these new paradigms but also the ability to generate code that appropriately incorporates them.\n",
    "* **Generic Responses:** Faced with queries about unfamiliar libraries, LLMs might revert to generating generic responses or code based on similar but outdated libraries. This not only leads to potentially incorrect code but also misses the opportunity to exploit the full capabilities of the new tools.\n",
    "\n",
    "To navigate these limitations effectively and ensure that LLMs can provide valuable assistance even with new libraries, consider the following approaches:\n",
    "\n",
    "* **Supplementing LLM Output with Current Documentation:** Users should cross-reference LLM-generated code with the most recent documentation available for the new library. This helps verify the accuracy of the code and ensures that it aligns with the latest specifications and best practices.\n",
    "Incorporating Human Expertise: Combining the efficiency of LLMs in generating initial drafts with the expertise of human developers can lead to more reliable and effective use of new libraries. Human oversight is crucial to adjust and refine the code, ensuring it leverages the full potential of the new library.\n",
    "* **Prompt Engineering:** Improving the prompts given to LLMs can help extract better outputs even when dealing with new libraries. Specific prompts that detail the functionality needed and any known aspects of the new library can guide the LLM to generate more relevant and useful code.\n",
    "Continuous Learning and Model Updating: Regularly updating the training data of LLMs to include the latest libraries, their documentation, and sample implementations can gradually improve their performance in generating code that effectively uses new tools.\n",
    "\n",
    "While LLMs bring considerable advantages to the table in code generation, their efficacy in handling newly released libraries is notably hindered by the limitations of their training data and inherent design. Addressing these challenges through strategic intervention and continuous learning is key to maximizing their utility in a rapidly evolving technological landscape.\n",
    "\n",
    "## Large proprietary function libraries\n",
    "\n",
    "Large Language Models (LLMs) like GPT have revolutionized many aspects of coding, but they encounter specific challenges when it comes to working with large proprietary function libraries that are common in enterprise environments:\n",
    "\n",
    "* **Lack of Exposure to Proprietary Data:** One of the primary limitations of LLMs is their training on publicly available data sets, which do not include proprietary or enterprise-specific libraries. Consequently, LLMs are generally unfamiliar with custom functions, classes, or methods developed within a specific organization, leading to outputs that might not align with internal tools or standards.\n",
    "\n",
    "* **Inadequacy in Understanding Enterprise-Specific Contexts:** Enterprises often have unique coding standards, naming conventions, and architectural patterns that are not publicly documented. LLMs, lacking exposure to these enterprise-specific practices, struggle to generate code that integrates seamlessly with existing internal codebases or adheres to the nuanced requirements of the organization.\n",
    "\n",
    "Proprietary libraries in enterprises often involve complex dependencies and interactions that are not typical of standard open-source libraries:\n",
    "\n",
    "* **Difficulty in Mapping Complex Interdependencies:** In large enterprises, proprietary libraries might interact in complex ways that require a deep understanding of the business logic and the overall system architecture. LLMs, operating largely on the syntactic and superficial semantic understanding of code, may find it challenging to appropriately handle these interactions or to optimize code involving multiple proprietary components.\n",
    "* **Generic Coding Patterns Over Custom Solutions:** Without specific training on an organization’s libraries, LLMs tend to revert to more generic coding patterns that may not leverage the full capabilities of proprietary libraries, potentially leading to less efficient or secure code.\n",
    "\n",
    "To effectively leverage LLMs in environments dominated by proprietary libraries, organizations can adopt several strategies:\n",
    "\n",
    "* **Custom Model Training:** Where possible, training a bespoke LLM on an organization's codebases and documentation can dramatically improve its ability to generate useful code snippets and solutions that are tailored to the proprietary systems.\n",
    "* **Enhanced Prompt Engineering:** Crafting detailed prompts that specify the context, desired outputs, and any relevant constraints can help guide the LLM to produce more appropriate and effective code. This includes explicitly mentioning any proprietary functions or classes that should be used in the solution.\n",
    "* **Hybrid Approaches with Human Oversight:** Integrating LLM-generated code with human review processes can ensure that the outputs align with enterprise standards and effectively utilize proprietary libraries. This approach also allows for the gradual refinement of the model’s understanding based on feedback and corrections from human developers.\n",
    "* **Documentation and Knowledge Base Integration:** Enhancing the LLM's access to internal documentation and knowledge bases through natural language queries can help bridge the knowledge gap. This could involve developing interfaces that allow the LLM to query internal documents dynamically as part of the code generation process.\n",
    "\n",
    "The challenges posed by large proprietary function libraries in enterprise settings highlight specific limitations of LLMs when deployed in such environments. However, with strategic adjustments and enhancements, the utility of LLMs can be significantly improved, making them valuable partners in enterprise-level software development. The key is to blend their capabilities with tailored resources and human expertise to ensure alignment with internal standards and optimal utilization of proprietary technologies.\n",
    "\n",
    "## Modifying large monolithic software projects\n",
    "\n",
    "Large Language Models (LLMs) such as GPT offer powerful capabilities for generating and modifying code, yet they face distinct challenges when applied to large monolithic software projects commonly found in enterprise environments:\n",
    "\n",
    "* **Complexity and Scale:** Monolithic applications are characterized by their large, interconnected codebases where multiple components are tightly coupled and depend on shared state. This complexity and the sheer volume of code can overwhelm LLMs, which are typically optimized for handling smaller, more discrete tasks. Their limited context window restricts their ability to grasp the full scope of a large application, potentially leading to suggestions that are contextually inappropriate or incomplete.\n",
    "* **Understanding Interdependencies:** In monolithic architectures, changes to one part of the system can have unexpected repercussions elsewhere due to the tightly coupled nature of the components. LLMs, lacking a deep understanding of these interdependencies, might generate code modifications that are syntactically correct but semantically problematic, disrupting the system's overall functionality.\n",
    "\n",
    "Monolithic systems in enterprise settings often incorporate legacy code and established practices that may not align with modern programming standards:\n",
    "\n",
    "* **Handling Legacy Code:** Legacy code can be particularly challenging for LLMs due to its unique quirks, outdated practices, and lack of adherence to current coding standards, which may not be well-represented in the model's training data. This can lead to the generation of code that either does not integrate well with the existing codebase or fails to comply with older systems' operational nuances.\n",
    "* **Conformity with Enterprise Standards:** Enterprises often have strict coding standards and practices that all modifications need to adhere to. LLMs might not be aware of these customized standards unless specifically trained on them, potentially leading to suggestions that fail to meet organizational compliance requirements.\n",
    "\n",
    "To address these limitations and effectively employ LLMs for modifying monolithic software projects, several strategies can be implemented:\n",
    "\n",
    "* **Incremental Integration:** Instead of large-scale modifications, using LLMs for incremental changes can be more manageable and less risky. This approach allows developers to isolate and address potential issues more effectively without disrupting the entire system.\n",
    "* **Human Oversight and Review:** It is crucial for human developers to closely review and supervise any modifications suggested by LLMs, especially in a monolithic context. This oversight ensures that changes are not only technically accurate but also align with broader system dependencies and enterprise standards.\n",
    "* **Enhanced Training and Customization:** Training LLMs on specific enterprise codebases and documentation can improve their understanding and performance when working with monolithic systems. Including detailed examples of past modifications and their impacts can also help the model learn appropriate patterns of change within the specific context.\n",
    "* **Use of Detailed Prompts:** Providing LLMs with detailed prompts that include information about the specific parts of the system, relevant dependencies, and compliance requirements can help generate more accurate and appropriate modifications.\n",
    "\n",
    "While LLMs hold promise for automating aspects of software development, their application in modifying large monolithic software projects within enterprise environments presents significant challenges. These systems' complexity, scale, and specific operational constraints demand a careful and considered approach. Combining LLM capabilities with strategic human oversight and system-specific training can help mitigate risks and enhance the effectiveness of modifications in these complex environments.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
