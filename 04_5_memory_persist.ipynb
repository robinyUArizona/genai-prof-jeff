{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_04_5_memory_persist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 4: LangChain: Chat and Memory**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 4 Material\n",
    "\n",
    "* Part 4.1: LangChain Conversations [[Video]](https://www.youtube.com/watch?v=Effbhxq07Ag) [[Notebook]](t81_559_class_04_1_langchain_chat.ipynb)\n",
    "* Part 4.2: Conversation Buffer Window Memory [[Video]](https://www.youtube.com/watch?v=14RgiFVGfAA) [[Notebook]](t81_559_class_04_2_memory_buffer.ipynb)\n",
    "* Part 4.3: Conversation Token Buffer Memory [[Video]](https://www.youtube.com/watch?v=QTe5g2c3bSM) [[Notebook]](t81_559_class_04_3_memory_token.ipynb)\n",
    "* Part 4.4: Conversation Summary Memory [[Video]](https://www.youtube.com/watch?v=asZQ8Ktqmt8) [[Notebook]](t81_559_class_04_4_memory_summary.ipynb)\n",
    "* **Part 4.5: Persisting Langchain Memory** [[Video]](https://www.youtube.com/watch?v=sjCyqqOQcPA) [[Notebook]](t81_559_class_04_5_memory_persist.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "54665e47-e883-433d-ff27-8165abcf168d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 4.5: Persisting Langchain Memory\n",
    "\n",
    "In this section, we delve into the creation of an innovative utility class designed for managing chat interactions—aptly named ChatBot. This class stands as a cornerstone for applications that require robust handling of chat history, including features to load and save these interactions, as well as capabilities to undo or regenerate outputs. The architecture of ChatBot integrates advanced language models with structured templates to facilitate interactive conversations, ensuring a seamless and dynamic user experience.\n",
    "\n",
    "The core functionality of ChatBot revolves around its sophisticated integration of components from the langchain ecosystem. It utilizes a large language model for generating chat responses and another for summarizing conversations, complemented by a template that structures each interaction. Key features include maintaining an editable conversation history, implementing undo and regenerate operations for greater control over chat flows, and mechanisms to persist conversation states, enhancing the utility and flexibility of the chat interface. As we explore ChatBot, we will uncover how each component contributes to its overall capability to manage complex conversational contexts effectively.\n",
    "\n",
    "The code for this class follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import PromptTemplate\n",
    "from IPython.display import display_markdown\n",
    "import pickle\n",
    "\n",
    "DEFAULT_TEMPLATE = \"\"\"You are a helpful assistant. Format answers with markdown.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, llm_chat, llm_summary, template):\n",
    "        \"\"\"\n",
    "        Initializes the ChatBot with language models and a template for conversation.\n",
    "\n",
    "        :param llm_chat: A large language model for handling chat responses.\n",
    "        :param llm_summary: A large language model for summarizing conversations.\n",
    "        :param template: A string template defining the conversation structure.\n",
    "        \"\"\"\n",
    "        self.llm_chat = llm_chat\n",
    "        self.llm_summary = llm_summary\n",
    "        self.template = template\n",
    "        self.prompt_template = PromptTemplate(input_variables=[\"history\", \"input\"], template=self.template)\n",
    "\n",
    "        # Initialize memory and conversation chain\n",
    "        self.memory = ConversationSummaryMemory(llm=self.llm_summary)\n",
    "        self.conversation = ConversationChain(\n",
    "            prompt=self.prompt_template,\n",
    "            llm=self.llm_chat,\n",
    "            memory=self.memory,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        self.history = []\n",
    "\n",
    "    def converse(self, prompt):\n",
    "        \"\"\"\n",
    "        Processes a conversation prompt and updates the internal history and memory.\n",
    "\n",
    "        :param prompt: The input prompt from the user.\n",
    "        :return: The generated response from the language model.\n",
    "        \"\"\"\n",
    "        self.history.append([self.memory.buffer, prompt])\n",
    "        output = self.conversation.invoke(prompt)\n",
    "        return output['response']\n",
    "\n",
    "    def chat(self, prompt):\n",
    "        \"\"\"\n",
    "        Handles the full cycle of receiving a prompt, processing it, and displaying the result.\n",
    "\n",
    "        :param prompt: The input prompt from the user.\n",
    "        \"\"\"\n",
    "        print(f\"Human: {prompt}\")\n",
    "        output = self.converse(prompt)\n",
    "        display_markdown(output, raw=True)\n",
    "\n",
    "    def print_memory(self):\n",
    "        \"\"\"\n",
    "        Displays the current state of the conversation memory.\n",
    "        \"\"\"\n",
    "        print(\"**Memory:\")\n",
    "        print(self.memory.buffer)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"\n",
    "        Clears the conversation memory.\n",
    "        \"\"\"\n",
    "        self.memory.clear()\n",
    "\n",
    "    def undo(self):\n",
    "        \"\"\"\n",
    "        Reverts the conversation memory to the state before the last interaction.\n",
    "        \"\"\"\n",
    "        if len(self.history) > 0:\n",
    "            self.memory.buffer = self.history.pop()[0]\n",
    "        else:\n",
    "            print(\"Nothing to undo.\")\n",
    "\n",
    "    def regenerate(self):\n",
    "        \"\"\"\n",
    "        Re-executes the last undone interaction, effectively redoing an undo operation.\n",
    "        \"\"\"\n",
    "        if len(self.history) > 0:\n",
    "            self.memory.buffer, prompt = self.history.pop()\n",
    "            self.chat(prompt)\n",
    "        else:\n",
    "            print(\"Nothing to regenerate.\")\n",
    "\n",
    "    def save_history(self, file_path):\n",
    "        \"\"\"\n",
    "        Saves the conversation history to a file using pickle.\n",
    "\n",
    "        :param file_path: The file path where the history should be saved.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(self.history, f)\n",
    "\n",
    "    def load_history(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads the conversation history from a file using pickle.\n",
    "\n",
    "        :param file_path: The file path from which to load the history.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            self.history = pickle.load(f)\n",
    "            # Optionally reset the memory based on the last saved state\n",
    "            if self.history:\n",
    "                self.memory.buffer = self.history[-1][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zJA-AF8vBtW"
   },
   "source": [
    "## Testing the Chat Bot\n",
    "Before we see how this class was constructed, lets see how you can use it. The following shows a simple chat interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tXJAk_dAtuFb",
    "outputId": "eaf8236d-6f3d-4ad9-bc0f-bef02873364a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yrobi\\AppData\\Local\\Temp\\ipykernel_11532\\2203850189.py:33: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  self.conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, my name is Jeff.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello, Jeff! How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: I like coffee.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "That's great to hear, Jeff! Coffee can be such a delightful beverage. Do you have a favorite type of coffee or a preferred way to prepare it?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff. How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Do I like coffee?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, you mentioned earlier that you like coffee! Do you have a favorite type or a preferred way to prepare it?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "# Initialize the OpenAI LLM with your API key\n",
    "llm = ChatOpenAI(\n",
    "  model=MODEL,\n",
    "  temperature= 0.3,\n",
    "  n= 1)\n",
    "\n",
    "c = ChatBot(llm, llm, DEFAULT_TEMPLATE)\n",
    "\n",
    "c.chat(\"Hello, my name is Jeff.\")\n",
    "c.chat(\"I like coffee.\")\n",
    "c.chat(\"What is my name.\")\n",
    "c.chat(\"Do I like coffee?\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEut99OTt_AG"
   },
   "source": [
    "## Load and Save Chat Memory\n",
    "\n",
    "The chat bot allows the complete chat history and memory to be saved and loaded from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Z09iSHYvt8is",
    "outputId": "4ece6ca7-776f-49f7-ecea-0e7e535e04b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, my name is Jeff.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello, Jeff! How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: I like coffee.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "That's great to hear, Jeff! Coffee can be a wonderful way to start the day or take a break. Do you have a favorite type of coffee or a preferred brewing method?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff. How can I assist you today, Jeff?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Do I like coffee?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, you mentioned that you like coffee! What's your favorite type or brewing method?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "# Initialize the OpenAI LLM with your API key\n",
    "llm = ChatOpenAI(\n",
    "  model=MODEL,\n",
    "  temperature= 0.3,\n",
    "  n= 1)\n",
    "\n",
    "c = ChatBot(llm, llm, DEFAULT_TEMPLATE)\n",
    "c.chat(\"Hello, my name is Jeff.\")\n",
    "c.chat(\"I like coffee.\")\n",
    "c.chat(\"What is my name.\")\n",
    "c.chat(\"Do I like coffee?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ScF3OysvTWk"
   },
   "source": [
    "We now save the chat memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pllixtJUuDP0"
   },
   "outputs": [],
   "source": [
    "c.save_history('history.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUvlZe3FvW-0"
   },
   "source": [
    "You can now reload the chat memory; the bot should remember the discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "id": "O3O2kcEzuFvF",
    "outputId": "eb72fde1-1619-4c13-d329-21781e5ed96a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff. How can I assist you today, Jeff?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = ChatBot(llm, llm, DEFAULT_TEMPLATE)\n",
    "c.load_history('history.pkl')\n",
    "c.chat(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClPhLkGldhPt"
   },
   "source": [
    "## Undoing and Regenerating the Chat\n",
    "\n",
    "The chatbot allows us to undo parts of the conversation, and it forgets. We can also regenerate outputs to get a better response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "ydaqwgRiH4D6",
    "outputId": "11aaa405-6a00-4a82-e6d8-bcdf9adb6f45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, my name is Jeff.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello, Jeff! How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: I like coffee.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "That's great to hear, Jeff! Coffee can be such a delightful way to start the day or take a break. Do you have a favorite type of coffee or a preferred brewing method?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff. How can I assist you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Do I like coffee?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "That's a great question, Jeff! I don't have access to your personal preferences, but if you enjoy the rich flavor and aroma of coffee, then you might like it. Do you usually drink coffee, or are you considering trying it?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Do I like coffee?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "That's a great question, Jeff! Do you enjoy coffee, or are you more of a tea person? If you have a preference, I'd love to hear about it!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "# Initialize the OpenAI LLM with your API key\n",
    "llm = ChatOpenAI(\n",
    "  model=MODEL,\n",
    "  temperature= 0.3,\n",
    "  n= 1)\n",
    "\n",
    "c = ChatBot(llm, llm, DEFAULT_TEMPLATE)\n",
    "\n",
    "c.chat(\"Hello, my name is Jeff.\")\n",
    "c.chat(\"I like coffee.\")\n",
    "c.undo()\n",
    "c.chat(\"What is my name.\")\n",
    "c.chat(\"Do I like coffee?\")\n",
    "c.regenerate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrNDYZ5rw1e_"
   },
   "source": [
    "## Implementing the Chatbot\n",
    "\n",
    "In this section, we delve into the architecture of a sophisticated chatbot built using the LangChain and OpenAI libraries, encapsulated within the ChatBot class. The ChatBot class leverages two language models and a structured conversation template to engage users in a dynamic conversational flow.\n",
    "\n",
    "The class initiates by defining its basic components: llm_chat and llm_summary are large language models used for chat responses and conversation summarization, respectively. The template parameter defines the conversation's structure, guiding how interactions are formatted and presented. This template incorporates markdown formatting and placeholders for the conversation history and the latest user input, facilitating a clear and structured dialogue presentation.\n",
    "\n",
    "Upon instantiation, the ChatBot initializes various components. It constructs a PromptTemplate, which prepares the structure for input variables and the conversation template. The ConversationSummaryMemory, initialized with the llm_summary, acts as a dynamic storage for conversation excerpts, aiding in recalling context or summarizing past dialogues. This memory component is crucial for maintaining conversation continuity, especially in sessions that might span multiple interactions.\n",
    "\n",
    "The ConversationChain is another pivotal element, combining the prompt template with the llm_chat model to process and respond to user inputs. The chain's verbose mode is set to False to streamline output without extraneous system messages.\n",
    "\n",
    "Interaction with the ChatBot is primarily through the chat method, where users provide a prompt, and the bot displays the response after processing. Internally, the converse method updates the conversation history and uses the conversation chain to generate a response. This method's utility lies in its ability to append each interaction to the history log, ensuring all discourse components are retained for future reference or undo actions.\n",
    "\n",
    "Memory management functions like print_memory, clear_memory, and undo provide users and developers with tools to view, reset, or revert the conversation state, thus offering flexibility in managing the dialogue flow and content. Additional functionalities to save and load conversation history are provided through save_history and load_history, utilizing Python’s pickle module for object serialization.\n",
    "\n",
    "This class structure not only facilitates robust conversation handling and memory management but also showcases the integration of advanced language models in practical applications, demonstrating the potential of AI in enhancing interactive software solutions. Through such a detailed setup, ChatBot serves as a comprehensive framework for building interactive, memory-aware chatbot systems."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
