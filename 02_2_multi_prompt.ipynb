{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_02_2_multi_prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 2: Code Generation**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Module 2 Material\n",
    "\n",
    "* Part 2.1: Prompting for Code Generation [[Video]](https://www.youtube.com/watch?v=HVId6kYKKgQ) [[Notebook]](t81_559_class_02_1_dev.ipynb)\n",
    "* **Part 2.2: Handling Revision Prompts** [[Video]](https://www.youtube.com/watch?v=APpV46tplXA) [[Notebook]](t81_559_class_02_2_multi_prompt.ipynb)\n",
    "* Part 2.3: Using a LLM to Help Debug [[Video]](https://www.youtube.com/watch?v=VPqSNb38QK0) [[Notebook]](t81_559_class_02_3_llm_debug.ipynb)\n",
    "* Part 2.4: Tracking Prompts in Software Development [[Video]](https://www.youtube.com/watch?v=oUFUuYfvXZU) [[Notebook]](t81_559_class_02_4_software_eng.ipynb)\n",
    "* Part 2.5: Limits of LLM Code Generation [[Video]](https://www.youtube.com/watch?v=dKtRI0LZSyY) [[Notebook]](t81_559_class_02_5_code_gen_limits.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "0106e4b9-a88e-4422-8ce6-258b78fd230e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 2.2: Handling Revision Prompts\n",
    "\n",
    "Previously, we just sent one prompt to the LLM, which generated code. It is possible to perform this code more conversationally. In this module, we will see how to converse with the LLM to request changes to outputted code and even help the LLM to produce a more accurate model.\n",
    "\n",
    "We will also see that it might be beneficial to recreate your conversation as one single prompt that generates the final result. Keeping track of one prompt, rather than a conversation, that created your final code is more maintainable.\n",
    "\n",
    "## Conversational Code Generation\n",
    "\n",
    "We will introduce a more advanced code generation function that allows you to start the conversation to generate code and follow up with additional prompts if needed.\n",
    "\n",
    "In future modules, we will see how to create chatbots similar to this one. We will use the code I provided to generate your code for now. This generator uses a system prompt that requests that the generated code conform to the following:\n",
    "\n",
    "* Imports should be sorted\n",
    "* Code should conform to PEP-8 formatting\n",
    "* Do not mix uncompilable notes with code\n",
    "* Add comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import PromptTemplate\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "TEMPLATE = \"\"\"The following is a friendly conversation between a human and an\n",
    "AI to generate Python code. If you have notes about the code, place them before\n",
    "the code. Any nots about execution should follow the code. If you do mix any\n",
    "notes with the code, make them comments. Add proper comments to the code.\n",
    "Sort imports and follow PEP-8 formatting.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "Code Assistant:\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)\n",
    "\n",
    "def start_conversation():\n",
    "    # Initialize the OpenAI LLM with your API key\n",
    "    llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.0,\n",
    "        n=1\n",
    "    )\n",
    "\n",
    "    # Initialize memory and conversation\n",
    "    memory = ConversationBufferWindowMemory()\n",
    "    conversation = ConversationChain(\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return conversation\n",
    "\n",
    "def generate_code(conversation, prompt):\n",
    "    print(\"Model response:\")\n",
    "    output = conversation.invoke(prompt)\n",
    "    display_markdown(output['response'], raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClPhLkGldhPt"
   },
   "source": [
    "## First Attempt at an XOR Approximator\n",
    "\n",
    "We will construct a prompt that requests the LLM to generate a PyTorch neural network to approximate the [Exclusive Or](https://en.wikipedia.org/wiki/Exclusive_or). The truth table for the Exclusive Or (XOR) function is provided here:\n",
    "\n",
    "```\n",
    "0 XOR 0 = 0\n",
    "1 XOR 0 = 1\n",
    "0 XOR 1 = 1\n",
    "1 XOR 1 = 0\n",
    "```\n",
    "\n",
    "If given data, neural networks can learn to approximate functions, so let's create a PyTorch neural network to approximate the XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ydaqwgRiH4D6",
    "outputId": "9ef9cde0-190c-4c38-c2a2-f80e65bef76b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure! Below is a Python code snippet that demonstrates how to learn the XOR function using PyTorch. The code includes comments to explain each part of the process.\n",
       "\n",
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "\n",
       "# Define the XOR dataset\n",
       "# Input: 2 features (x1, x2), Output: 1 target (y)\n",
       "data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "labels = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
       "\n",
       "# Define a simple neural network model\n",
       "class XORModel(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(XORModel, self).__init__()\n",
       "        # Input layer to hidden layer with 2 neurons and ReLU activation\n",
       "        self.hidden = nn.Linear(2, 2)\n",
       "        # Hidden layer to output layer with 1 neuron\n",
       "        self.output = nn.Linear(2, 1)\n",
       "    \n",
       "    def forward(self, x):\n",
       "        # Pass input through the hidden layer and apply ReLU activation\n",
       "        x = torch.relu(self.hidden(x))\n",
       "        # Pass through the output layer\n",
       "        x = torch.sigmoid(self.output(x))\n",
       "        return x\n",
       "\n",
       "# Initialize the model, loss function, and optimizer\n",
       "model = XORModel()\n",
       "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
       "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
       "\n",
       "# Training the model\n",
       "num_epochs = 1000  # Number of epochs for training\n",
       "for epoch in range(num_epochs):\n",
       "    # Forward pass: compute predicted outputs by passing inputs to the model\n",
       "    outputs = model(data)\n",
       "    # Compute the loss\n",
       "    loss = criterion(outputs, labels)\n",
       "    \n",
       "    # Zero the gradients before the backward pass\n",
       "    optimizer.zero_grad()\n",
       "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
       "    loss.backward()\n",
       "    # Update the model parameters\n",
       "    optimizer.step()\n",
       "    \n",
       "    # Print the loss every 100 epochs\n",
       "    if (epoch + 1) % 100 == 0:\n",
       "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
       "\n",
       "# Test the model\n",
       "with torch.no_grad():\n",
       "    test_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "    predictions = model(test_data)\n",
       "    print(\"Predictions:\")\n",
       "    print(predictions.round())  # Round the predictions to get binary output\n",
       "```\n",
       "\n",
       "### Notes on Execution:\n",
       "- Ensure you have PyTorch installed in your Python environment. You can install it using pip:\n",
       "  ```bash\n",
       "  pip install torch\n",
       "  ```\n",
       "- The model is trained for 1000 epochs, but you can adjust this number based on your needs.\n",
       "- The output predictions are rounded to get binary values (0 or 1) for the XOR function.\n",
       "- The training loss is printed every 100 epochs to monitor the training process."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = start_conversation()\n",
    "generate_code(conversation, \"\"\"Write Python code to learn the XOR function with PyTorch.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hPqM7BslmM7"
   },
   "source": [
    "# Requesting a Change to Generated Code\n",
    "\n",
    "If you've taken my other course, you will know I prefer PyTorch sequences over extending the nn.Module class, at least for simple neural networks like an XOR approximator. LLMs do not share this opinion. However, the LLM will gladly humor me and generate a sequence. Here, I provide an additional prompt to request this rather than resubmitting a modified version of my first prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 948
    },
    "id": "lqnDZhc4OVU6",
    "outputId": "8ef3e8c4-68c7-4001-8708-efaceead6102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! Below is the modified Python code that uses a PyTorch `nn.Sequential` to define the model instead of creating a custom `nn.Module` class. This approach simplifies the model definition while still achieving the same functionality.\n",
       "\n",
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "\n",
       "# Define the XOR dataset\n",
       "# Input: 2 features (x1, x2), Output: 1 target (y)\n",
       "data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "labels = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
       "\n",
       "# Define a simple neural network model using nn.Sequential\n",
       "model = nn.Sequential(\n",
       "    nn.Linear(2, 2),  # Input layer to hidden layer with 2 neurons\n",
       "    nn.ReLU(),        # ReLU activation function\n",
       "    nn.Linear(2, 1),  # Hidden layer to output layer with 1 neuron\n",
       "    nn.Sigmoid()      # Sigmoid activation function for binary output\n",
       ")\n",
       "\n",
       "# Initialize the loss function and optimizer\n",
       "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
       "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
       "\n",
       "# Training the model\n",
       "num_epochs = 1000  # Number of epochs for training\n",
       "for epoch in range(num_epochs):\n",
       "    # Forward pass: compute predicted outputs by passing inputs to the model\n",
       "    outputs = model(data)\n",
       "    # Compute the loss\n",
       "    loss = criterion(outputs, labels)\n",
       "    \n",
       "    # Zero the gradients before the backward pass\n",
       "    optimizer.zero_grad()\n",
       "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
       "    loss.backward()\n",
       "    # Update the model parameters\n",
       "    optimizer.step()\n",
       "    \n",
       "    # Print the loss every 100 epochs\n",
       "    if (epoch + 1) % 100 == 0:\n",
       "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
       "\n",
       "# Test the model\n",
       "with torch.no_grad():\n",
       "    test_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "    predictions = model(test_data)\n",
       "    print(\"Predictions:\")\n",
       "    print(predictions.round())  # Round the predictions to get binary output\n",
       "```\n",
       "\n",
       "### Notes on Execution:\n",
       "- Ensure you have PyTorch installed in your Python environment. You can install it using pip:\n",
       "  ```bash\n",
       "  pip install torch\n",
       "  ```\n",
       "- The model is trained for 1000 epochs, but you can adjust this number based on your needs.\n",
       "- The output predictions are rounded to get binary values (0 or 1) for the XOR function.\n",
       "- The training loss is printed every 100 epochs to monitor the training process."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_code(conversation, \"\"\"Could you make use of a PyTorch sequence rather than a nn.Module class?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7U1Bx3Wbje1"
   },
   "source": [
    "# Testing the Generated Code\n",
    "\n",
    "LLMs are not overachievers; they will implement the code you ask for and not provide much more. When we run the XOR approximator's first version, the results are only sometimes accurate, especially if we run the program multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdJNKLVhRcD3",
    "outputId": "7e690772-13d7-4ec7-813a-64b426bb8855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.6921\n",
      "Epoch [200/1000], Loss: 0.6692\n",
      "Epoch [300/1000], Loss: 0.5869\n",
      "Epoch [400/1000], Loss: 0.4809\n",
      "Epoch [500/1000], Loss: 0.3667\n",
      "Epoch [600/1000], Loss: 0.2569\n",
      "Epoch [700/1000], Loss: 0.1769\n",
      "Epoch [800/1000], Loss: 0.1255\n",
      "Epoch [900/1000], Loss: 0.0934\n",
      "Epoch [1000/1000], Loss: 0.0724\n",
      "Predictions:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the XOR dataset\n",
    "# Input: 2 features (x1, x2), Output: 1 target (y)\n",
    "data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "labels = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define a simple neural network model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),  # Input layer to hidden layer with 2 neurons\n",
    "    nn.ReLU(),        # ReLU activation function\n",
    "    nn.Linear(2, 1),  # Hidden layer to output layer with 1 neuron\n",
    "    nn.Sigmoid()      # Sigmoid activation function for binary output\n",
    ")\n",
    "\n",
    "# Initialize the loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 1000  # Number of epochs for training\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "    outputs = model(data)\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Zero the gradients before the backward pass\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    test_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "    predictions = model(test_data)\n",
    "    print(\"Predictions:\")\n",
    "    print(predictions.round())  # Round the predictions to get binary output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY-NmeRAoPVr"
   },
   "source": [
    "If you receive an error or the output is not exactly what you like, it is effective to provide that output and any errors to the LLM. Here, we provide the output and ask the LLM if that seems correct. Sometimes, the LLM may insist that the output is correct, so you must \"debate\" the LLM, providing additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bzkCRwx_RxxI",
    "outputId": "e9c3628f-97e4-4c56-9082-ec38719f5f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The output you received indicates that the model has not yet learned the XOR function effectively. This can happen for several reasons, such as insufficient training epochs, a learning rate that is too high or too low, or the model architecture not being complex enough to capture the XOR relationship.\n",
       "\n",
       "To improve the model's performance, you can try the following adjustments:\n",
       "\n",
       "1. **Increase the number of hidden neurons**: The XOR function is not linearly separable, so a more complex model may be needed.\n",
       "2. **Change the learning rate**: Experiment with different learning rates to see if the model converges better.\n",
       "3. **Increase the number of training epochs**: Sometimes, the model needs more time to learn.\n",
       "\n",
       "Here's an updated version of the code with a more complex model architecture and an increased number of epochs:\n",
       "\n",
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "\n",
       "# Define the XOR dataset\n",
       "# Input: 2 features (x1, x2), Output: 1 target (y)\n",
       "data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "labels = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
       "\n",
       "# Define a more complex neural network model using nn.Sequential\n",
       "model = nn.Sequential(\n",
       "    nn.Linear(2, 4),  # Input layer to hidden layer with 4 neurons\n",
       "    nn.ReLU(),        # ReLU activation function\n",
       "    nn.Linear(4, 2),  # Hidden layer to another hidden layer with 2 neurons\n",
       "    nn.ReLU(),        # ReLU activation function\n",
       "    nn.Linear(2, 1),  # Hidden layer to output layer with 1 neuron\n",
       "    nn.Sigmoid()      # Sigmoid activation function for binary output\n",
       ")\n",
       "\n",
       "# Initialize the loss function and optimizer\n",
       "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
       "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
       "\n",
       "# Training the model\n",
       "num_epochs = 5000  # Increased number of epochs for training\n",
       "for epoch in range(num_epochs):\n",
       "    # Forward pass: compute predicted outputs by passing inputs to the model\n",
       "    outputs = model(data)\n",
       "    # Compute the loss\n",
       "    loss = criterion(outputs, labels)\n",
       "    \n",
       "    # Zero the gradients before the backward pass\n",
       "    optimizer.zero_grad()\n",
       "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
       "    loss.backward()\n",
       "    # Update the model parameters\n",
       "    optimizer.step()\n",
       "    \n",
       "    # Print the loss every 500 epochs\n",
       "    if (epoch + 1) % 500 == 0:\n",
       "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
       "\n",
       "# Test the model\n",
       "with torch.no_grad():\n",
       "    test_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "    predictions = model(test_data)\n",
       "    print(\"Predictions:\")\n",
       "    print(predictions.round())  # Round the predictions to get binary output\n",
       "```\n",
       "\n",
       "### Notes on Execution:\n",
       "- Ensure you have PyTorch installed in your Python environment. You can install it using pip:\n",
       "  ```bash\n",
       "  pip install torch\n",
       "  ```\n",
       "- The model architecture has been modified to include an additional hidden layer and more neurons, which should help it learn the XOR function better.\n",
       "- The number of training epochs has been increased to 5000 to give the model more time to learn.\n",
       "- The output predictions are rounded to get binary values (0 or 1) for the XOR function.\n",
       "- The training loss is printed every 500 epochs to monitor the training process."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_code(conversation, \"\"\"The output was:\n",
    "\n",
    "Predicted values:\n",
    "tensor([[0.4843],\n",
    "        [0.5800],\n",
    "        [0.4278],\n",
    "        [0.4623]])\n",
    "\n",
    "Are you sure that is correct?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r2Qpe11cWwm"
   },
   "source": [
    "## Test the Improved Version\n",
    "\n",
    "We now receive much more accurate output when we test the neural network provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V19QyaROSUP1",
    "outputId": "8e8993aa-6d11-4788-b6e5-3d291c8097de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/5000], Loss: 0.0549\n",
      "Epoch [1000/5000], Loss: 0.0177\n",
      "Epoch [1500/5000], Loss: 0.0101\n",
      "Epoch [2000/5000], Loss: 0.0070\n",
      "Epoch [2500/5000], Loss: 0.0054\n",
      "Epoch [3000/5000], Loss: 0.0044\n",
      "Epoch [3500/5000], Loss: 0.0037\n",
      "Epoch [4000/5000], Loss: 0.0031\n",
      "Epoch [4500/5000], Loss: 0.0028\n",
      "Epoch [5000/5000], Loss: 0.0025\n",
      "Predictions:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the XOR dataset\n",
    "# Input: 2 features (x1, x2), Output: 1 target (y)\n",
    "data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "labels = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define a more complex neural network model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 4),  # Input layer to hidden layer with 4 neurons\n",
    "    nn.ReLU(),        # ReLU activation function\n",
    "    nn.Linear(4, 2),  # Hidden layer to another hidden layer with 2 neurons\n",
    "    nn.ReLU(),        # ReLU activation function\n",
    "    nn.Linear(2, 1),  # Hidden layer to output layer with 1 neuron\n",
    "    nn.Sigmoid()      # Sigmoid activation function for binary output\n",
    ")\n",
    "\n",
    "# Initialize the loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 5000  # Increased number of epochs for training\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "    outputs = model(data)\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Zero the gradients before the backward pass\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss every 500 epochs\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    test_data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "    predictions = model(test_data)\n",
    "    print(\"Predictions:\")\n",
    "    print(predictions.round())  # Round the predictions to get binary output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCAmOVHEpAjf"
   },
   "source": [
    "## Combining the Conversation into a Single Prompt\n",
    "\n",
    "We should combine this entire conversation into a single prompt, especially if we wish to save the prompt along with the code. We can request the LLM to create this combined prompt for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "MKMcmXTNSiFw",
    "outputId": "4f81e0cd-57b3-4581-e0e9-18d129663384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Certainly! A single prompt that would have led to the last code output could be:\n",
       "\n",
       "\"Write Python code using PyTorch to learn the XOR function with a more complex neural network architecture, including an additional hidden layer and more neurons. Train the model for 5000 epochs and print the loss every 500 epochs. Ensure to use nn.Sequential for model definition and include comments explaining each part of the code.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_code(conversation, \"\"\"Okay, that is great, can you suggest a single\n",
    "prompt that would have resulted in this last code output?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw-QDu7Rpo_T"
   },
   "source": [
    "The LLM's attempt at a consoldated prompt is incomplete. It skips several important details and does not provide precise requirements. I will manually make some improvements, which you can see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1TzyiT6aS7ej",
    "outputId": "13c3d633-d5c7-4c57-8787-581ed09b2618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure! Below is the Python code that uses PyTorch to learn the XOR function with 4 hidden neurons, the Adam optimizer, and 20,000 training epochs. The code is structured as a sequence rather than using a `nn.Module` class.\n",
       "\n",
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "\n",
       "# Set the random seed for reproducibility\n",
       "torch.manual_seed(0)\n",
       "\n",
       "# Define the XOR input and output\n",
       "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
       "\n",
       "# Define the neural network architecture\n",
       "# Input layer (2 neurons) -> Hidden layer (4 neurons) -> Output layer (1 neuron)\n",
       "input_size = 2\n",
       "hidden_size = 4\n",
       "output_size = 1\n",
       "\n",
       "# Initialize weights and biases\n",
       "W1 = torch.randn(input_size, hidden_size, requires_grad=True) * 0.1  # Weights for input to hidden\n",
       "b1 = torch.zeros(hidden_size, requires_grad=True)  # Bias for hidden layer\n",
       "W2 = torch.randn(hidden_size, output_size, requires_grad=True) * 0.1  # Weights for hidden to output\n",
       "b2 = torch.zeros(output_size, requires_grad=True)  # Bias for output layer\n",
       "\n",
       "# Define the learning rate and number of epochs\n",
       "learning_rate = 0.01\n",
       "epochs = 20000\n",
       "\n",
       "# Training loop\n",
       "for epoch in range(epochs):\n",
       "    # Forward pass: compute predicted y\n",
       "    hidden_layer = torch.sigmoid(X @ W1 + b1)  # Activation for hidden layer\n",
       "    output_layer = torch.sigmoid(hidden_layer @ W2 + b2)  # Activation for output layer\n",
       "\n",
       "    # Compute the loss (Mean Squared Error)\n",
       "    loss = nn.MSELoss()(output_layer, y)\n",
       "\n",
       "    # Backward pass: compute gradients\n",
       "    loss.backward()\n",
       "\n",
       "    # Update weights and biases using Adam optimizer\n",
       "    with torch.no_grad():\n",
       "        W1 -= learning_rate * W1.grad\n",
       "        b1 -= learning_rate * b1.grad\n",
       "        W2 -= learning_rate * W2.grad\n",
       "        b2 -= learning_rate * b2.grad\n",
       "\n",
       "        # Zero the gradients after updating\n",
       "        W1.grad.zero_()\n",
       "        b1.grad.zero_()\n",
       "        W2.grad.zero_()\n",
       "        b2.grad.zero_()\n",
       "\n",
       "    # Print the loss every 2000 epochs\n",
       "    if epoch % 2000 == 0:\n",
       "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
       "\n",
       "# Test the trained model\n",
       "with torch.no_grad():\n",
       "    test_output = torch.sigmoid(torch.sigmoid(X @ W1 + b1) @ W2 + b2)\n",
       "    print(\"Test Output:\")\n",
       "    print(test_output)\n",
       "```\n",
       "\n",
       "### Notes on Execution:\n",
       "- Ensure you have PyTorch installed in your Python environment. You can install it using pip if you haven't done so already: `pip install torch`.\n",
       "- The code initializes weights and biases randomly and uses the sigmoid activation function for both the hidden and output layers.\n",
       "- The loss is calculated using Mean Squared Error (MSE), and the model is trained for 20,000 epochs.\n",
       "- The output of the model is printed after training, showing how well it has learned the XOR function."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start a new conversation\n",
    "conversation = start_conversation()\n",
    "generate_code(conversation, \"\"\"\n",
    "Can you provide Python code using PyTorch to effectively learn the XOR function\n",
    "with 4 hidden neurons, using the Adam optimizer, and 20K training epochs?\n",
    "Use a sequence not a nn.Module class.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EiMd01ATosU"
   },
   "source": [
    "## Test the Final Prompt\n",
    "\n",
    "Now, we test the final prompt. My prompt produces an acceptable result, but there are some opportunities for improvement. You can specify the exact format for the output. For example, sometimes code is generated to round the results, but other times it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fx0_Al1kTmh7",
    "outputId": "131bc687-76a4-4ec4-cb62-0ef08e815be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/20000], Loss: 0.0034\n",
      "Epoch [2000/20000], Loss: 0.0009\n",
      "Epoch [3000/20000], Loss: 0.0004\n",
      "Epoch [4000/20000], Loss: 0.0002\n",
      "Epoch [5000/20000], Loss: 0.0001\n",
      "Epoch [6000/20000], Loss: 0.0001\n",
      "Epoch [7000/20000], Loss: 0.0000\n",
      "Epoch [8000/20000], Loss: 0.0000\n",
      "Epoch [9000/20000], Loss: 0.0000\n",
      "Epoch [10000/20000], Loss: 0.0000\n",
      "Epoch [11000/20000], Loss: 0.0000\n",
      "Epoch [12000/20000], Loss: 0.0000\n",
      "Epoch [13000/20000], Loss: 0.0000\n",
      "Epoch [14000/20000], Loss: 0.0000\n",
      "Epoch [15000/20000], Loss: 0.0000\n",
      "Epoch [16000/20000], Loss: 0.0000\n",
      "Epoch [17000/20000], Loss: 0.0000\n",
      "Epoch [18000/20000], Loss: 0.0000\n",
      "Epoch [19000/20000], Loss: 0.0000\n",
      "Epoch [20000/20000], Loss: 0.0000\n",
      "Predicted tensor: tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "Actual tensor: tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the XOR inputs and outputs\n",
    "inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float)\n",
    "targets = torch.tensor([[0], [1], [1], [0]], dtype=torch.float)\n",
    "\n",
    "# Define the model using a sequential container\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 4),  # Input layer to hidden layer with 4 neurons\n",
    "    nn.ReLU(),        # ReLU activation function\n",
    "    nn.Linear(4, 1),  # Hidden layer to output layer\n",
    "    nn.Sigmoid()      # Sigmoid activation function for binary output\n",
    ")\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer with learning rate of 0.01\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20000):  # 20,000 training epochs\n",
    "    optimizer.zero_grad()   # Clear gradients for each training step\n",
    "    outputs = model(inputs)  # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "    loss = criterion(outputs, targets)  # Compute loss\n",
    "    loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/20000], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "with torch.no_grad():  # Context-manager that disabled gradient calculation\n",
    "    predicted = model(inputs).round()  # Forward pass and rounding off to get predictions\n",
    "    print(f'Predicted tensor: {predicted}')\n",
    "    print(f'Actual tensor: {targets}')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
