{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_generative_ai/blob/main/t81_559_class_03_5_book.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# T81-559: Applications of Generative Artificial Intelligence\n",
    "**Module 3: Large Language Models**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4Yov72PhstY"
   },
   "source": [
    "# Module 3 Material\n",
    "\n",
    "* Part 3.1: Foundation Models [[Video]](https://www.youtube.com/watch?v=Gb0tk5qq1fA) [[Notebook]](t81_559_class_03_1_llm.ipynb)\n",
    "* Part 3.2: Text Generation [[Video]](https://www.youtube.com/watch?v=lB97Lqt7q58) [[Notebook]](t81_559_class_03_2_text_gen.ipynb)\n",
    "* Part 3.3: Text Summarization [[Video]](https://www.youtube.com/watch?v=3MoIUXE2eEU) [[Notebook]](t81_559_class_03_3_text_summary.ipynb)\n",
    "* Part 3.4: Text Classification [[Video]](https://www.youtube.com/watch?v=2VpOwFIGmA8) [[Notebook]](t81_559_class_03_4_classification.ipynb)\n",
    "* **Part 3.5: LLM Writes a Book** [[Video]](https://www.youtube.com/watch?v=iU40Rttlb_Q) [[Notebook]](t81_559_class_03_5_book.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcAUP0c3hstY"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsI496h5hstZ",
    "outputId": "3dc269f8-d90a-4f51-cc22-7affe4557fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: not using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive, userdata\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "# OpenAI Secrets\n",
    "if COLAB:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# Install needed libraries in CoLab\n",
    "if COLAB:\n",
    "    !pip install langchain langchain_openai langchain_community pypdf pdfkit\n",
    "    !apt-get install wkhtmltopdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 3.5: LLM Writes a Book\n",
    "\n",
    "This section will demonstrate how to effectively utilize the LLM to write a book. Writing a book is a uniquely complex challenge that requires planning and execution. Unlike simpler tasks, drafting a full-length book is a journey that extends beyond a single session with an LLM due to its extensive length and depth. However, with a systematic approach, you can harness the capabilities of an LLM to create a comprehensive manuscript, empowering you to conquer this challenge.\n",
    "\n",
    "This section presents a method to craft a book using an LLM, which involves breaking down the book creation process into manageable segments. Our journey begins with a foundational step: selecting a subject for the book. Once a subject is chosen, the process unfolds through a series of iterative interactions with the LLM, each building upon the last to gradually construct the complete book, igniting your passion for writing.\n",
    "\n",
    "Initially, we prompt the LLM to generate a random title based on the given subject. This title sets the thematic tone for the book and guides the subsequent steps. Following the title, we request the LLM to create a random synopsis. This synopsis acts as a narrative backbone, offering a brief overview of the book’s content and direction.\n",
    "\n",
    "With a synopsis, the next step is to generate a table of contents. This table outlines the main chapters and sections of the book, providing a structured framework that organizes the content logically and cohesively. It is from this framework that the book starts to take shape.\n",
    "The next phase involves creating each chapter. We engage in a focused session with the LLM for every chapter outlined in the table of contents. During each session, we provide the LLM with the synopsis and the complete table of contents to ensure consistency and continuity throughout the book. This iterative process allows each chapter to be crafted with attention to detail, ensuring it aligns with the previously established narrative and thematic elements.\n",
    "\n",
    "By methodically iterating through these steps, we harness the capabilities of an LLM to transform a simple idea into a detailed and well-structured book. This chapter will guide you through each stage, providing practical advice on collaborating with an LLM to achieve more complex tasks.\n",
    "\n",
    "We begin by accessing a large language model with a temperature of 0.7. We use a higher temperature to encourage creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dfISNTpwOKiZ"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.7,\n",
    "        n=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLUWbW7NRp4m"
   },
   "source": [
    "We create a simple utility function to query the LLM with a custom system prompt. The system prompt tells the LLM that it is assisting in creating a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v8vGrZcjTvwZ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def query_llm(prompt):\n",
    "  messages = [\n",
    "      SystemMessage(\n",
    "          content=\"You are assistant helping to write a book.\"\n",
    "      ),\n",
    "      HumanMessage(\n",
    "          content=prompt\n",
    "      ),\n",
    "  ]\n",
    "\n",
    "  output = llm.invoke(messages)\n",
    "  return output.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOWb0XT7RlYg"
   },
   "source": [
    "## Generate Title, Synopsis and Table of Contents\n",
    "\n",
    "For this book, we allow the user to specify the subject specified by the SUBJECT variable. We then request the LLM to generate a random title based on this subject. It is essential that the prompt request that the LLM only the; LLMs often like to prefix with text such as \"Here is a random title.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpEgTYV1TvpH",
    "outputId": "d5f348e7-6bbb-458e-f60d-f6c8eddba0e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shadows of Deceit\n"
     ]
    }
   ],
   "source": [
    "SUBJECT = \"international spy thriller\"\n",
    "\n",
    "title = query_llm(f\"\"\"\n",
    "Give me a random title for a book on the subject '{SUBJECT}'.\n",
    "Return only the title, no additional text.\n",
    "\"\"\").strip(\" '\\\"\")\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXyOF_oYtMlR"
   },
   "source": [
    "Now that we have a title, we can request a random synopsis of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxYdLzYCTvgQ",
    "outputId": "99bcaade-468b-4188-d808-a4cbd6b74739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world where trust is a luxury and secrets are currency, former MI6 agent Elara Reed is pulled back into the shadows after a high-profile assassination shatters the delicate balance of power among the world’s intelligence agencies. With a cryptic warning from her estranged mentor and a rogue faction of spies threatening global security, Elara races against time to uncover a conspiracy that spans continents. As she navigates a treacherous web of double agents, high-stakes espionage, and political intrigue, she must rely on her instincts and unearth hidden truths that could change the course of history. Betrayal lurks at every corner, and as alliances shift, Elara must confront her own past to prevent a catastrophic event that could ignite a world war. In a pulse-pounding chase from London to Moscow, she discovers that the most dangerous game is played not just with weapons, but with the secrets of the heart.\n"
     ]
    }
   ],
   "source": [
    "synopsis = query_llm(f\"\"\"\n",
    "Give me a synopsis for a book of the title '{SUBJECT}' for a book on the subject '{SUBJECT}'.\n",
    "Return only the synopsis, no additional text.\n",
    "\"\"\").strip(\" '\\\"\")\n",
    "print(synopsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6ju-lJEttjw"
   },
   "source": [
    "Next, we generate the table of contents. For this generation, we provide all previous information. We also request a particular format for the table of contents. You may notice that I ask for the chapter numbers, even though they are an increasing numeric count and could easily be derived. This process works better because the LLM wants to provide the chapter numbers, and attempts to suppress chapter numbers are often inconsistent. It is easier to allow the LLM to generate chapter numbers but control where it generates them so that I can consistently remove them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IhXP9PJTvXq",
    "outputId": "813f24ee-1379-4cc0-b26e-69fee44c5122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | Shadows of the Past  \n",
      "2 | A Fateful Encounter  \n",
      "3 | The Ashes of Betrayal  \n",
      "4 | Cryptic Warnings  \n",
      "5 | The Call to Action  \n",
      "6 | In the Crosshairs  \n",
      "7 | A Web of Lies  \n",
      "8 | The Hidden Enemy  \n",
      "9 | Race Against Time  \n",
      "10 | Allies and Adversaries  \n",
      "11 | The Heart of Darkness  \n",
      "12 | Secrets of the Heart  \n",
      "13 | The City of Spies  \n",
      "14 | Unraveling Threads  \n",
      "15 | The Brink of War  \n",
      "16 | Collateral Damage  \n",
      "17 | The Final Gambit  \n",
      "18 | Betrayal Revealed  \n",
      "19 | A Dangerous Game  \n",
      "20 | Echoes of Deceit  \n",
      "21 | The Last Stand  \n",
      "22 | A New Dawn\n"
     ]
    }
   ],
   "source": [
    "toc = query_llm(f\"\"\"\n",
    "Give me a table of contents for a book of the title '{title}' for a book on\n",
    "the subject '{SUBJECT}' the book synopsis is '{synopsis}'.\n",
    "Return the table of contents as a list of chapter titles.\n",
    "Separate the chapter number and chapter title with a pipe character '|'.\n",
    "Return only the chapter names, no additional text.\n",
    "\"\"\").strip(\" '\\\"\")\n",
    "print(toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvh6hlVBCN2f"
   },
   "source": [
    "We must now parse the table of contents and remove the pipes and chapter numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-A4RgyR3Mid",
    "outputId": "e7d15614-454f-4bd8-ed2f-ac122253f0ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shadows of the Past', 'A Fateful Encounter', 'The Ashes of Betrayal', 'Cryptic Warnings', 'The Call to Action', 'In the Crosshairs', 'A Web of Lies', 'The Hidden Enemy', 'Race Against Time', 'Allies and Adversaries', 'The Heart of Darkness', 'Secrets of the Heart', 'The City of Spies', 'Unraveling Threads', 'The Brink of War', 'Collateral Damage', 'The Final Gambit', 'Betrayal Revealed', 'A Dangerous Game', 'Echoes of Deceit', 'The Last Stand', 'A New Dawn']\n"
     ]
    }
   ],
   "source": [
    "# Split the string into lines\n",
    "lines = toc.splitlines()\n",
    "\n",
    "# Extract titles using list comprehension\n",
    "toc2 = [line.split('|')[1].strip() for line in lines if line]\n",
    "\n",
    "# Print the list of titles\n",
    "print(toc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMXLYGi_RxNp"
   },
   "source": [
    "## Generate the Chapters of the Book\n",
    "\n",
    "Next, we create a function capable of producing the text that makes up a chapter. To ensure that the function has enough context to generate each chapter, we provide the synopsis, the table of contents, and the chapter number. To test this code, we request that it develop a single chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATXhVI9NfUTQ",
    "outputId": "db5a8fc5-364e-40c9-98c8-037064f57f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elara Reed stood at the window of her modest apartment in London, the city sprawled beneath her like a living entity, pulsing with life and secrets. The rain drummed against the glass, a steady rhythm that matched the thumping beat of her heart. It had been years since she traded her life as an MI6 agent for the quietude of anonymity, yet the shadows of her past loomed large, threatening to engulf her once more.\n",
      "\n",
      "The news blared from the television in the corner, a multitude of voices clamoring for attention. \"High-profile assassination rocks the intelligence community,\" the anchor announced, her face a mask of concern. Elara's gaze sharpened as the details unfolded. A prominent diplomat had been killed in a brazen attack, an act of violence that sent ripples through the already fragile alliances of the world’s intelligence agencies. She felt a chill creep up her spine, her instincts honed from years of training whispering that this was no ordinary killing.\n",
      "\n",
      "Memories flooded back, uninvited and relentless. Faces of old friends and foes danced in her mind—her mentor, Victor Hargrove, with his silver hair and piercing blue eyes; the way he had taught her to navigate the murky waters of espionage. The last time she saw him, their parting had been acrimonious. He had warned her about the dangers of trusting too easily, of letting her guard down. Those words echoed now, a haunting reminder of the world she had tried to leave behind.\n",
      "\n",
      "A soft ping interrupted her thoughts. She turned to her phone, the screen illuminating a message that sent her heart racing. It was from an unknown number, but the urgency of the words caught her breath: “Elara, they’re coming for you. Trust no one.” \n",
      "\n",
      "Her pulse quickened as she scanned her surroundings. The once-familiar apartment felt foreign, a sanctuary turned potential trap. She had made a life for herself away from the chaos, but the past had a way of clawing back, dragging her into the darkness she thought she had escaped.\n",
      "\n",
      "With a deep breath, Elara forced herself to focus. She grabbed her coat, slipping it on over her shoulders. She needed to move, to think. The city was alive outside, and she would blend in, become just another shadow among many. But as she stepped into the rain-soaked streets, a sense of foreboding settled in her gut. The world had changed since she walked away from the game, and she was no longer certain who she could trust.\n",
      "\n",
      "As she navigated the familiar streets of London, the faces around her blurred into a sea of anonymity. Yet, she couldn’t shake the feeling of being watched. Her instincts, finely tuned during her years in the field, screamed at her to remain vigilant. She ducked into a café, the warm air wrapping around her like a blanket, but the comfort was fleeting. \n",
      "\n",
      "Sipping her coffee, she scanned the room. A man in a dark overcoat sat in the corner, his eyes fixed on her. She felt the weight of his gaze, a chill creeping back into her bones. Was he a friend or foe? The line between the two had always been razor-thin in her world. \n",
      "\n",
      "Deciding she couldn’t linger, Elara stood up, her mind racing. The message, the assassination, the man in the corner—it all pointed to something larger at play, a conspiracy that threatened to unravel the delicate balance of power. She needed answers, and she knew just where to start. \n",
      "\n",
      "With purpose, she stepped back into the rain, the city’s heartbeat matching her own as she set off toward the one person who might still be able to help her—Victor Hargrove. Whether he would welcome her with open arms or turn her away was a risk she had to take. The shadows of the past were closing in, and she could no longer afford the luxury of hesitation.\n"
     ]
    }
   ],
   "source": [
    "def render_chapter(num, chapter_title, title, subject, synopsis, toc):\n",
    "  txt = query_llm(f\"\"\"\n",
    "  Write Chapter {num}, titled \"{chapter_title}\" for a book of the title '{title}' for a book on\n",
    "  the subject '{subject}' the book synopsis is '{synopsis}' the table of contents is '{toc}'.\n",
    "  Give me only the chapter text, no chapter heading, no chapter title, number, no additional text.\n",
    "  \"\"\").strip(\" '\\\"\")\n",
    "  return txt\n",
    "\n",
    "txt = render_chapter(1, toc2[0], title, SUBJECT, synopsis, toc)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6s7Igm5cSf50"
   },
   "source": [
    "We can now generate the entire book in Markdown, which allows some formatting. We begin by rendering the title and synopsis, the table of contents, and each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uq8xv3RIBkIv",
    "outputId": "e325aace-e375-469d-895d-bee3129adef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering chapter 1/22: Shadows of the Past\n",
      "Rendering chapter 2/22: A Fateful Encounter\n",
      "Rendering chapter 3/22: The Ashes of Betrayal\n",
      "Rendering chapter 4/22: Cryptic Warnings\n",
      "Rendering chapter 5/22: The Call to Action\n",
      "Rendering chapter 6/22: In the Crosshairs\n",
      "Rendering chapter 7/22: A Web of Lies\n",
      "Rendering chapter 8/22: The Hidden Enemy\n",
      "Rendering chapter 9/22: Race Against Time\n",
      "Rendering chapter 10/22: Allies and Adversaries\n",
      "Rendering chapter 11/22: The Heart of Darkness\n",
      "Rendering chapter 12/22: Secrets of the Heart\n",
      "Rendering chapter 13/22: The City of Spies\n",
      "Rendering chapter 14/22: Unraveling Threads\n",
      "Rendering chapter 15/22: The Brink of War\n",
      "Rendering chapter 16/22: Collateral Damage\n",
      "Rendering chapter 17/22: The Final Gambit\n",
      "Rendering chapter 18/22: Betrayal Revealed\n",
      "Rendering chapter 19/22: A Dangerous Game\n",
      "Rendering chapter 20/22: Echoes of Deceit\n",
      "Rendering chapter 21/22: The Last Stand\n",
      "Rendering chapter 22/22: A New Dawn\n"
     ]
    }
   ],
   "source": [
    "book = \"\"\n",
    "\n",
    "# Render the title and synopsis\n",
    "book += f\"# {title}\\n\"\n",
    "book += f\"{synopsis}\\n\"\n",
    "\n",
    "# Render the toc\n",
    "book += f\"\\n## Table of Contents\\n\\n\"\n",
    "num = 1\n",
    "for chapter_title in toc2:\n",
    "  book += f\"{num}. {chapter_title}\\n\"\n",
    "  num += 1\n",
    "\n",
    "# Render the book\n",
    "chapter = 1\n",
    "for chapter_title in toc2:\n",
    "  print(f\"Rendering chapter {chapter}/{len(toc2)}: {chapter_title}\")\n",
    "  txt = render_chapter(chapter, chapter_title, title, SUBJECT, synopsis, toc)\n",
    "  book += f\"\\n\\n## Chapter {chapter}: {chapter_title}\\n\"\n",
    "  book += f\"{txt}\\n\"\n",
    "  chapter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXVa0IxKR5V7"
   },
   "source": [
    "## Generate a PDF of the Book\n",
    "\n",
    "Now that we have generated the book, we have saved it as a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJNvL2xsfVoF",
    "outputId": "44262bfa-88af-4e7b-f536-47d68bd48cfe"
   },
   "outputs": [],
   "source": [
    "# import markdown\n",
    "# import pdfkit\n",
    "\n",
    "# # Convert Markdown to HTML\n",
    "# html = markdown.markdown(book)\n",
    "# options = {\n",
    "#     'encoding': \"UTF-8\",  # Ensures UTF-8 encoding\n",
    "# }\n",
    "\n",
    "# pdfkit.from_string(html, 'output.pdf', options=options)\n",
    "\n",
    "\n",
    "# import markdown\n",
    "# from weasyprint import HTML\n",
    "\n",
    "# # Sample Markdown content (you can replace this with your actual content)\n",
    "# book = \"# Sample Markdown Content\\nThis is a sample content.\"\n",
    "\n",
    "# # Convert Markdown to HTML\n",
    "# html_content = markdown.markdown(book)\n",
    "\n",
    "# # Convert HTML to PDF using WeasyPrint\n",
    "# HTML(string=html_content).write_pdf('output.pdf')\n",
    "\n",
    "# print(\"PDF generated successfully: output.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTsMT8sYYh6C"
   },
   "source": [
    "We can now download the generated book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "pqcpXCsABZZo",
    "outputId": "14814401-c301-411f-b906-e6cf481b99be"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_f9b78315-fc30-4709-93d9-fdeadebec978\", \"output.pdf\", 32042)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from google.colab import files\n",
    "# files.download(\"output.pdf\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
